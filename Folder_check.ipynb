{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all needed functions\n",
    "import os\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "from astropy.io import fits\n",
    "import glob\n",
    "from Calculating_det_angles import estimate_source_angles_detectors  # Importing ma'am's function\n",
    "from Tools import tools\n",
    "import threading\n",
    "from queue import Queue\n",
    "import queue\n",
    "import traceback\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3492\n",
      "3492\n",
      "events already downloaded\n",
      " {'GRB': 1000, 'SFLARE': 1000, 'SGR': 492, 'TGF': 1000}\n",
      "events to download\n",
      " {}\n"
     ]
    }
   ],
   "source": [
    "# reading the ttsplit file\n",
    "df = pd.read_csv('ttsplit')\n",
    "\n",
    "# getting the test and train events\n",
    "test_events = df[df['category'] == 'test']['filename'].tolist()\n",
    "\n",
    "train_events = df[df['category'] == 'train']['filename'].tolist()\n",
    "\n",
    "# loading the event names and event types in the list\n",
    "\n",
    "event_names = []\n",
    "event_types = []\n",
    "\n",
    "for _,row in df.iterrows():\n",
    "    if (row[\"category\"] == \"test\" or row[\"category\"] == \"train\") and row[\"filename\"][-1] != 'd':\n",
    "        event_type,event_name = row['filename'].split(\"_\")\n",
    "        event_types.append(event_type)\n",
    "        event_names.append(event_name)\n",
    "\n",
    "print(len(event_names))\n",
    "print(len(event_types))\n",
    "\n",
    "\n",
    "data_set_path = r\"D:\\GRB_data\\new_dataset\"\n",
    "\n",
    "# Get a list of all folders in the specified directory\n",
    "folders = [folder for folder in os.listdir(data_set_path) if os.path.isdir(os.path.join(data_set_path, folder))]\n",
    "\n",
    "# Create sets for faster lookup\n",
    "folder_set = set(folders)\n",
    "event_types = set(['GRB', 'SFLARE', 'TGF', 'SGR'])\n",
    "\n",
    "# Count events already downloaded\n",
    "downloaded_events = Counter(folder.split('_')[0] for folder in folders if folder.split('_')[0] in event_types)\n",
    "print(\"events already downloaded\\n\", dict(downloaded_events))\n",
    "\n",
    "# Create set of existing event names for faster lookup\n",
    "existing_event_names = set(folder.split('_')[1] for folder in folders)\n",
    "\n",
    "# Initialize counters for events to download\n",
    "events_to_download = Counter()\n",
    "\n",
    "# Lists to store new events\n",
    "new_event_types = []\n",
    "new_event_names = []\n",
    "\n",
    "event_limit = 100000\n",
    "for _, row in df.iterrows():\n",
    "    event_type,event_name = row['filename'].split(\"_\")\n",
    "    if (row[\"category\"] == \"test\" or row[\"category\"] == \"train\") and row[\"filename\"][-1] != 'd':\n",
    "        if event_type in event_types and event_name not in existing_event_names:\n",
    "            if events_to_download[event_type] < event_limit - downloaded_events[event_type]:\n",
    "                events_to_download[event_type] += 1\n",
    "                new_event_types.append(event_type)\n",
    "                new_event_names.append(event_name)\n",
    "\n",
    "print('events to download\\n', dict(events_to_download))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker thread 0 started.1\n",
      "\n",
      "Worker thread 0 processing event SGR GRB_bn080714086.\n",
      "C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\temp0\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\temp0' already exists.\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\temp0' and its contents removed successfully.\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\temp0' created successfully.\n",
      "download start\n",
      "Error downloading the file: Command 'wget -q -nH --no-check-certificate --cut-dirs=7 -r -l0 -c -N -np -A \"*_trigdat_*\" -R \"index*\" -erobots=off --retr-symlinks https://heasarc.gsfc.nasa.gov/FTP/fermi/data/gbm/triggers/20B_/GRB_bn080714086/current/ -P C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\temp0' returned non-zero exit status 8.\n",
      "Error:\n",
      "        - Event: GRB_bn080714086\n",
      "        - Exception: IndexError\n",
      "        - Message: list index out of range\n",
      "        - Details: list index out of range\n",
      "        - Traceback:\n",
      "        Traceback (most recent call last):\n",
      "  File \"C:\\Users\\arpan\\AppData\\Local\\Temp\\ipykernel_19012\\2438878874.py\", line 23, in process_event\n",
      "    trigdat_filename = trigdat_file[0]\n",
      "IndexError: list index out of range\n",
      "\n",
      "        - An error occurred during processing.\n",
      "Worker thread 0 processed event GRB_bn080714086.\n",
      "Worker thread 0 finished.\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_path = tools.json_path(r'data_path.json')\n",
    "\n",
    "# Function to process a single event\n",
    "def process_event(event_name,thread_num):\n",
    "    try:\n",
    "        temp_thread = r'temp'+ str(thread_num)\n",
    "        temp_path = os.path.join(dir_path, temp_thread)\n",
    "        print(temp_path)\n",
    "        tools.create_folder(temp_path)\n",
    "        event = event_name\n",
    "        year = '20' + event[2:4] + \"/\"\n",
    "\n",
    "        # URL of the file you want to download : TRIGDAT file\n",
    "        url = 'wget -q -nH --no-check-certificate --cut-dirs=7 -r -l0 -c -N -np -A \"*_trigdat_*\" -R \"index*\" -erobots=off --retr-symlinks https://heasarc.gsfc.nasa.gov/FTP/fermi/data/gbm/triggers/' + year + event + '/current/'\n",
    "        tools.run_wget_download(url, temp_path)\n",
    "\n",
    "        # Finding Trigdat file\n",
    "        trig_string = \"_trigdat_\"\n",
    "        trig_pattern = os.path.join(temp_path, 'current', f\"*{trig_string}*\")\n",
    "        trigdat_file = glob.glob(trig_pattern)\n",
    "\n",
    "        # Get the spacecraft pointing from here\n",
    "        trigdat_filename = trigdat_file[0]\n",
    "\n",
    "        # URL of the file you want to download : .rsp file\n",
    "        url = 'wget -q -nH --no-check-certificate --cut-dirs=7 -r -l0 -c -N -np -A \"*.rsp*\" -R \"index*\" -erobots=off --retr-symlinks https://heasarc.gsfc.nasa.gov/FTP/fermi/data/gbm/triggers/' + year + event + '/current/'\n",
    "        tools.run_wget_download(url, temp_path)\n",
    "\n",
    "        # Finding .rsp file\n",
    "        rsp_string = \".rsp\"\n",
    "        rsp_pattern = os.path.join(temp_path, 'current', f\"*{trig_string}*\")\n",
    "        rsp_file = glob.glob(trig_pattern)\n",
    "\n",
    "        # Getting the RA and DEC\n",
    "        with fits.open(rsp_file[0], memmap=True) as pha_list:\n",
    "            ra_obj, dec_obj = (pha_list[0].header['RA_OBJ']), (pha_list[0].header['DEC_OBJ'])\n",
    "\n",
    "        brightest_nai, bright_nais, brightest_bgo,nai_angles = estimate_source_angles_detectors.angle_to_grb(ra_obj, dec_obj,trigdat_filename)\n",
    "\n",
    "        # Get a list of all files in the folder\n",
    "        files = [file for file in os.listdir(data_set_path) if not os.path.isdir(os.path.join(data_set_path, file))]\n",
    "\n",
    "        # find the needed nais\n",
    "        nais = []\n",
    "        for nai_angle,nai in zip(nai_angles,bright_nais):\n",
    "            if nai == brightest_nai or int(nai_angle) < 60: # only download the brightest detector or if the detector has anlge less than 60 deg\n",
    "                nais.append(nai)\n",
    "\n",
    "        nai_flags = []\n",
    "        for nai in nais:\n",
    "            flag = False\n",
    "            for file in files:\n",
    "                if nai in file:\n",
    "                    flag = True\n",
    "            nai_flags.append(flag)\n",
    "        \n",
    "        print(nai_flags)\n",
    "            \n",
    "        # Print the size of the queue\n",
    "        print(event_queue.qsize()) \n",
    "\n",
    "    except Exception as e:\n",
    "        # Get formatted traceback\n",
    "        traceback_str = traceback.format_exc()\n",
    "\n",
    "        # Access exception attributes (example)\n",
    "        if isinstance(e, ZeroDivisionError):\n",
    "            error_details = \"Division by zero attempted.\"\n",
    "        else:\n",
    "            error_details = str(e.args[0]) if len(e.args) else \"Unknown error.\"\n",
    "\n",
    "        # Custom message based on exception type\n",
    "        if isinstance(e, ValueError):\n",
    "            custom_message = \"Invalid value provided. Check your input data.\"\n",
    "        else:\n",
    "            custom_message = \"An error occurred during processing.\"\n",
    "\n",
    "        # Print detailed message\n",
    "        print(f\"\"\"Error:\n",
    "        - Event: {event_name}\n",
    "        - Exception: {type(e).__name__}\n",
    "        - Message: {e}\n",
    "        - Details: {error_details}\n",
    "        - Traceback:\n",
    "        {traceback_str}\n",
    "        - {custom_message}\"\"\")\n",
    "\n",
    "# Create a queue to store the events\n",
    "event_queue = Queue()\n",
    "\n",
    "# Function to run the worker threads\n",
    "def worker(thread_num):\n",
    "    print(f\"Worker thread {thread_num} started.\")\n",
    "\n",
    "    while True : #\n",
    "        # Get an event from the queue\n",
    "        event_name = event_queue.get()\n",
    "        print(f\"Worker thread {thread_num} processing event {event_type} {event_name}.\")\n",
    "\n",
    "        # Process the event\n",
    "        process_event(event_name, thread_num)\n",
    "\n",
    "        # Signal that the task is completed\n",
    "        event_queue.task_done()\n",
    "        print(f\"Worker thread {thread_num} processed event {event_name}.\")\n",
    "\n",
    "        if event_queue.qsize() == 0:\n",
    "            break\n",
    "    \n",
    "    print(f\"Worker thread {thread_num} finished.\")\n",
    "    \n",
    "# Create and start the worker threads\n",
    "num_threads = 1\n",
    "threads = []\n",
    "for i in range(num_threads):\n",
    "    worker_thread = threading.Thread(target=worker, args=(i,), daemon=True)\n",
    "    worker_thread.start()\n",
    "    threads.append(worker_thread)\n",
    "\n",
    "# Add events to the queue\n",
    "for folder in folders[:1]:\n",
    "    event_queue.put((folder))\n",
    "\n",
    "print(event_queue.qsize()) \n",
    "\n",
    "# # Wait for all events to be processed\n",
    "# event_queue.join()\n",
    "# print(\"All events have been processed.\")\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
