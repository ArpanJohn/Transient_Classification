{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9167495,"sourceType":"datasetVersion","datasetId":5539445},{"sourceId":9179381,"sourceType":"datasetVersion","datasetId":5547999},{"sourceId":9183201,"sourceType":"datasetVersion","datasetId":5550656},{"sourceId":9194823,"sourceType":"datasetVersion","datasetId":5558590}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport os\nimport glob\nimport matplotlib.pyplot as plt\nimport pathlib as Path\nimport json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-18T09:42:25.774168Z","iopub.execute_input":"2024-08-18T09:42:25.774881Z","iopub.status.idle":"2024-08-18T09:42:25.780056Z","shell.execute_reply.started":"2024-08-18T09:42:25.774836Z","shell.execute_reply":"2024-08-18T09:42:25.778925Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def find_json_file(directory):\n    \"\"\"\n    Find a JSON file in a directory (and its subdirectories) and load it into a dictionary.\n    \n    Parameters:\n        directory (str): Path to the directory to search.\n    \n    Returns:\n        dict or None: Dictionary containing the JSON data if a file is found, \n                      None otherwise.\n    \"\"\"\n    for root, dirs, files in os.walk(directory):\n        for file_name in files:\n            if file_name.endswith('.json'):\n                file_path = os.path.join(root, file_name)\n                with open(file_path, 'r') as file:\n                    try:\n                        json_data = json.load(file)\n                        return json_data\n                    except json.JSONDecodeError:\n                        print(f\"Error: JSON decode failed for file '{file_path}'\")\n    print(\"No JSON file found in the directory.\")\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.781756Z","iopub.execute_input":"2024-08-18T09:42:25.782145Z","iopub.status.idle":"2024-08-18T09:42:25.791109Z","shell.execute_reply.started":"2024-08-18T09:42:25.782080Z","shell.execute_reply":"2024-08-18T09:42:25.790070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Path\ndir_path = r\"/kaggle/input/dcl-ds2/DCL_ds2\"   \n\n# Check if the path exists\nif os.path.exists(dir_path):\n    print(f\"The path '{dir_path}' exists.\")\nelse:\n    print(f\"The path '{dir_path}' does not exist.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.792822Z","iopub.execute_input":"2024-08-18T09:42:25.793217Z","iopub.status.idle":"2024-08-18T09:42:25.803969Z","shell.execute_reply.started":"2024-08-18T09:42:25.793186Z","shell.execute_reply":"2024-08-18T09:42:25.802821Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The path '/kaggle/input/dcl-ds2/DCL_ds2' exists.\n","output_type":"stream"}]},{"cell_type":"code","source":"# importing data\nevent_types = {1:'GRB',2:'TGF',3:'SGR',4:'SFLARE'}\n\ntest_dir = os.path.join(dir_path,\"test\")  \ntrain_dir = os.path.join(dir_path,\"train\")\n\n# Replace 'your_search_string' with the string you are looking for in file names\nsearch_string = 'bn'\n\nsearch_pattern = os.path.join(test_dir, f'*{search_string}*')\ntest_files = glob.glob(search_pattern)\n\nsearch_pattern = os.path.join(train_dir, f'*{search_string}*')\ntrain_files = glob.glob(search_pattern)\n\njson_data = find_json_file(dir_path)\nprint(json_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.805721Z","iopub.execute_input":"2024-08-18T09:42:25.806278Z","iopub.status.idle":"2024-08-18T09:42:25.834693Z","shell.execute_reply.started":"2024-08-18T09:42:25.806245Z","shell.execute_reply":"2024-08-18T09:42:25.833820Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'bin list': [0.004, 0.016, 0.064, 0.256, 1.024, 4.096], 'time interval': 'n/a', 'number of data points': 999, 'data set name': 'DCL_ds2', 'data set path': 'C:\\\\Users\\\\arpan\\\\OneDrive\\\\Documents\\\\GRB\\\\data\\\\DCL_ds2', 'channel ranges': [[3, 50], [51, 124]]}\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(test_files))\nprint(test_files[:10])\n\nprint(len(train_files))\nprint(train_files[:10])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.837012Z","iopub.execute_input":"2024-08-18T09:42:25.837255Z","iopub.status.idle":"2024-08-18T09:42:25.842258Z","shell.execute_reply.started":"2024-08-18T09:42:25.837233Z","shell.execute_reply":"2024-08-18T09:42:25.841339Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"784\n['/kaggle/input/dcl-ds2/DCL_ds2/test/GRB_bn170113420', '/kaggle/input/dcl-ds2/DCL_ds2/test/SFLARE_bn141216438', '/kaggle/input/dcl-ds2/DCL_ds2/test/SFLARE_bn131029420', '/kaggle/input/dcl-ds2/DCL_ds2/test/SGR_bn221013037d', '/kaggle/input/dcl-ds2/DCL_ds2/test/GRB_bn171010875', '/kaggle/input/dcl-ds2/DCL_ds2/test/SGR_bn160715298', '/kaggle/input/dcl-ds2/DCL_ds2/test/GRB_bn220407167', '/kaggle/input/dcl-ds2/DCL_ds2/test/TGF_bn140628393', '/kaggle/input/dcl-ds2/DCL_ds2/test/TGF_bn180607061', '/kaggle/input/dcl-ds2/DCL_ds2/test/TGF_bn180922713']\n3200\n['/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160626724', '/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn230911329', '/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160416676', '/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn150322066', '/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120312671', '/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110606097', '/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn100212307', '/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140110411', '/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn131025607', '/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150531687']\n","output_type":"stream"}]},{"cell_type":"code","source":"dno = json_data['number of data points']\n\ntry:\n    chrn = len(json_data['channel ranges'])\nexcept:\n    chrn = 1\n    \ntry:\n    t = json_data['time interval'][1]-json_data['time interval'][0]\nexcept:\n    t='n/a'\n    \n# Calculating the number of points in each binnning\nbin_list = json_data['bin list']\ndno_list = (chrn * len(bin_list)) * [dno]\ndata_total = sum(dno_list)\ncheck_data = np.loadtxt(train_files[1], delimiter='\\t').astype(np.int32)\nlen_data = len(check_data)\n\nprint(dno_list)\nif data_total == len_data and len(dno_list) == len(bin_list) * chrn:\n    print('bin edges calculated correctly')\nelse:\n    print('inconsistency in bin edges calculated and data')\n    print(data_total)\n    print(len_data)\n    \nprint('channel range no',chrn)\nprint('number of data points',dno)\nprint('time interval',t)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.843469Z","iopub.execute_input":"2024-08-18T09:42:25.843731Z","iopub.status.idle":"2024-08-18T09:42:25.859275Z","shell.execute_reply.started":"2024-08-18T09:42:25.843707Z","shell.execute_reply":"2024-08-18T09:42:25.858227Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999]\nbin edges calculated correctly\nchannel range no 2\nnumber of data points 999\ntime interval n/a\n","output_type":"stream"}]},{"cell_type":"code","source":"zerofiles = []\nc = 0\nfor file in train_files:\n    check = np.loadtxt(file, delimiter='\\t').astype(np.int32)\n    if np.all(check == 0):\n        c +=1\n        zerofiles.append(file)\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:25.860579Z","iopub.execute_input":"2024-08-18T09:42:25.860914Z","iopub.status.idle":"2024-08-18T09:42:42.942808Z","shell.execute_reply.started":"2024-08-18T09:42:25.860884Z","shell.execute_reply":"2024-08-18T09:42:42.941804Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"425\n","output_type":"stream"}]},{"cell_type":"code","source":"for line in zerofiles:\n    print(line)","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:42.943950Z","iopub.execute_input":"2024-08-18T09:42:42.944225Z","iopub.status.idle":"2024-08-18T09:42:42.951502Z","shell.execute_reply.started":"2024-08-18T09:42:42.944200Z","shell.execute_reply":"2024-08-18T09:42:42.950622Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160626724\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110606097\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn230702969\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180709467\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101226778\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230525064\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn130909817\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101219539\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121123712\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn090522190\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130416162\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100912943\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100604105\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190514901\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140907429\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140526571\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101230452\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220622088\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120117291\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn201123719d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120604082\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn160428412\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210412791\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130510430\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn130113632\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170530108\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120306365\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141204407\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100901312\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091213945\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140826460\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100501128\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120706785\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230507862\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210324485\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn200203156\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140715983\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220903773\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160430349\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110601852\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111207484\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200715712\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210505305\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161203745\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110119456\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190221692\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn131023220\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120114380\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190613961\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200903230\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190826301\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn151002266\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn190806535\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180815065\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100305806\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180906868\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160505869\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn201130203\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200205836\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170804714\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn171119681\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121125826\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220530248\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110927453\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200821416\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170728844\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200810021\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn200718350\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230331948\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101025783\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120708662\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091211599\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn110124784\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn221109865\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170509539\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn150106921\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230323413\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn111227502\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn140404157\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn230730552\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130124463\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230701878\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn110911071\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111126752\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120705047\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn210913827\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230724873\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180627844\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230802316\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn211006877\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101022830\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn111227092\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140807514\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120705596\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121031293\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230901482\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn140114144\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121017715\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn101029730\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn111104936\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130505422\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230417860\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100826543\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn111002906\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160519024d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140308747\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160501904\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn151210379\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180929781\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110928729\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn191104056d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230610254\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn151025721\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn210213417\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170910491\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230330966\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101128617\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn201029938\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn220114168\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130502410\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121210138\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190304208\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150525449\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100927599\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn080831053\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn210913827d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140125521\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090212968\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn221109865d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130525418\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180425518\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161104977\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170320675\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100920577\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200404327\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210821332\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100925319\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn130325005\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141218784\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140724533\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn171024210\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121227687\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220704026\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090203805\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100513287\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120911417\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn181229647\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240204302\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161114056\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150603062\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100306199\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120212353\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn181002333\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn171223392\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn150311331\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150413702\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090212968d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn221003016\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091213783\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn100208133\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100207721\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220418286\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220606558\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121212349\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150430558\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200913091\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140714392\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn211225932\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn111001804\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160824468\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101009134\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210722034\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn151111631\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn211113838\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240301991\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170128756\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140721187\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190225451\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230527635\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220215724\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230913220\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn131016605\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220108612\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn131024809\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091130288\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240323566\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111212461\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn121108098\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn090606471\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140929111\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140423311\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110212898\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn191202988\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120304540\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110801661\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180531403\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170425071\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120727354\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230801655\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220113437d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120311143\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111024345\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn101011707\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn230415474\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230110892\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170811359\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn110107970\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200318226\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210320788\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130316390\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100216663\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230807586\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100609597\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120430163\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100210761\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110929614\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140813479\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230604742\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120605851\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180907966\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120925954\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn191104056\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120509361\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn221212368\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220109206\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn090808739\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn191214677\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn181009128\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170411927\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180614673\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150905034\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161130235\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140402007\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161102289\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn231018389\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn240310563\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180619279\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160826587\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180530478\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100202802\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170619358\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220108612d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn151110680\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn200718350d\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160519024\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190509787\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn220523365\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170621896\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220203820\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230409785\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn130111258\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120510340\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190705344\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn101225377\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn140429551\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141022874\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121214906\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130803857\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180814995\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120114433\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160826587d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190930439\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120706011\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn150917401\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120809447\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180610903\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn140214559\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111119856\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141013086\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn230718188\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn220606044\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180413498\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170923773\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn110517453\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230810343\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141116877\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn131205795\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140520699\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120707445\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn131006367\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn231214705\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140616165\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140529201\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn211011761\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110906986\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100409543\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn231029583\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100911473\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100225345\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101002507\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170220864\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn200203156d\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090203805d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180510906\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110214810\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230605796\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200122398\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170609358\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161213977\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180106416\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn091019750\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150327463\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170718524\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn201119634\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn230623782\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160704661\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190712633\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100527795\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160801464\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090401093d\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn221014437d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161211001\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100610132\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111121780\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110923918\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn111207512\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240122823\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121008333\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn101002279\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn211220171\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180709140\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn140518709\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220120628d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170618654\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn231014399\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160424403\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101115695\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100726397\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn230304569\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn171030532\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100110328\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240324093\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150714555\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn161021007\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200312245\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190417464\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn130628984\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210617554\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200630518\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110926973\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100126460\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn100505720\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn211201765\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn191012787\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn211225932d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120727939\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn221001560\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121028243\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110521783\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn191023414\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150918289\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190907631\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn171124565\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140830603\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110307858\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140905648\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111210978\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn170629303\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091213876\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100705663\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn120926753\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn120118798\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100728940\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn160128498\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn140320360\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220113437\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150416485\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn090307167\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190604577\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220219401\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn160503887\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120530347\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn131102562\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn221127794\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101114700\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn221014437\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180522659\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn141029398\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140901588\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100616773\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180724548\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn110107970d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn091224757\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn110224817\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn200630076\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn190806065\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn210213417d\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn101010190\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220109206d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn140623885\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn160626724d\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100617497\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn220120628\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn101213283\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn090401093\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn150824651\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn111023352\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120430757\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn190825171\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn210425468\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn201123719\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn181110809\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110522841\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180116162\n/kaggle/input/dcl-ds2/DCL_ds2/train/SGR_bn140429551d\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240207003\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn231130359\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100225374\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180103442\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120428015\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn180626653\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn131121989\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn210912522\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn121101287\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn100516396\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn100223288\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn150820209\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn220519495\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn200226521\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn151002956\n/kaggle/input/dcl-ds2/DCL_ds2/train/SFLARE_bn240219532\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120312139\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn110709173\n/kaggle/input/dcl-ds2/DCL_ds2/train/TGF_bn120728461\n/kaggle/input/dcl-ds2/DCL_ds2/train/GRB_bn101119685\n","output_type":"stream"}]},{"cell_type":"code","source":"f = 0\nfor i in dno_list:\n    print(check_data[f:f+i])\n    f =f + i","metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":true},"execution":{"iopub.status.busy":"2024-08-18T09:42:42.955193Z","iopub.execute_input":"2024-08-18T09:42:42.955634Z","iopub.status.idle":"2024-08-18T09:42:43.001450Z","shell.execute_reply.started":"2024-08-18T09:42:42.955600Z","shell.execute_reply":"2024-08-18T09:42:43.000602Z"},"collapsed":true,"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0   41   40    0   39\n    0   39   38   38   38  287  287    0    0   36  286   35   35   35\n   34   34  284  533   33   33    0   32  282    0   31    0  780   30\n  279   29   29  278    0    0    0    0   27   26   26    0   25  275\n   25   24   24  274   23    0  273   22   22  271  521    0    0  270\n    0    0    0   19    0    0   18   17    0   17    0  516    0  765\n   15  515   14    0  264   13   13   12  262  262  261   11  511    0\n  510  260  509    9    9    0    0    0    0  257  757    0    6    0\n  255    0    5    0  254    3  253    3    2    0    0  251  251  251\n    0  250    0    0    0    0  248    0    0    0    0  497  496    0\n    0    0    0  244  494    0    0    0    0    0    0    0    0  491\n    0  240  740  490  489    0    0    0    0    0  237    0  487    0\n    0    0    0    0  234    0  234    0    0    0    0    0    0    0\n  231    0  230  230    0    0    0  479    0    0  228    0    0  726\n  226    0    0    0  475    0    0    0    0    0    0    0    0  222\n    0  221  221    0    0    0    0    0    0    0  218  217    0  217\n    0    0  716    0    0    0    0  214  214    0  213  213    0  212\n    0    0    0    0    0  210    0  209  209  458    0    0    0    0\n    0    0    0    0    0  205  455    0    0    0    0  203  453    0\n    0    0    0    0  201    0    0    0    0    0  198  448    0    0\n    0  197    0  446    0  195    0    0  194    0    0    0    0    0\n    0    0    0  191    0  190    0  190    0    0    0    0    0    0\n    0    0    0    0    0    0  185    0    0    0  434    0  433  183\n    0  182  182    0  181    0  180    0    0    0    0    0    0    0\n  178    0  177    0    0  676    0    0    0  175    0    0    0    0\n    0    0    0    0    0    0  171    0    0  170    0    0    0    0\n  168    0    0    0  167    0    0    0    0    0  415  414    0    0\n  163    0  412  162 1412    0    0    0    0    0  160    0    0  409\n  158    0  408  157    0    0  156    0  156    0    0    0    0    0\n  153  403    0  152  152    0  151  151    0    0    0    0    0  149\n    0    0    0    0    0    0    0  146    0    0    0    0  144  644\n    0  143    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0   67\n  128    2    0    0  123  372    0  370   56  118    0  303  114  488\n  174   48    0  295    0   43    0    0  226    0    0    0    0  345\n   93  279    0    0    0  211  148  146    0   18    0  328    0  263\n    0    0  633  444    0  316    0   63   62    0    0  120    0    0\n    0  301  362  173  484    0    0    0    0  726    0  223    0    0\n  468    0  214  150    0  272    0  206   17  203    0    0    0   71\n    0  255    0  314  250    0    0   57    0 1054  239  488    0  171\n  170    0   41    0  350    0    0  344    0  216    0  650    0    0\n    0    0    0  138  262    0    0  381    0  127  375   61    0    0\n    0    0  176  112  110    0    0  416  789    0    0  284    0  405\n  153  213    0  272    0    0    0    0  137  197  320    0    3    0\n  437    0    0   56   54  302    0    0    0    0   41    0  162    0\n    0    0    0    0    0    0  708    0   16  202    0   73  383  568\n    0    0    0    0  370  306    0    0  362  172  108  230    0   39\n  411  472   32    0    0    0    0  271    0    0   15   75    0  196\n    0  504    0    0    0   58  118  179  176  362    0    0   43    0\n  351    0  284    0    0  340  463    0   83    0    0    0  325  510\n    0  630  691    0    0  497  619    0    0   50   48  483  106  104\n    0  100  347  470  343    0   26    0    0   20    0   78  450    0\n   71    0    0  127    0    0    0  181    0    0  112   47  170    0\n    0  163  224  284    0    0    0    0    0  208    0    0  452    0\n  197    0    6  503    1  374    0  245  118    0  113    0    0    0\n  105    0    0    0  221    0  279    0    0    0  333    0   16    0\n   75   73  258    0    4    0  124    0    0    0  741  364    0  235\n    0    0    0    0    0   97    0    0    0    0  274    0    0    0\n    0    0    0    0  446    0  316    0    0    0    0    0    0   52\n   50  173  234  169    0    0    0    0  785    0    0  216   27    0\n    0  146    0  267  390    0   74    0    0    0   66  314    0    0\n  183    0    0    0  238    0    0    0   44  229    0    0    0   97\n    0    0    0  277  213    0    0    0    0    0   15    0    0    0\n    0  131    4    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0  280    0    0    0\n    0    0    0    0    0  203    0    0    0    0   71    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0  162    0    0    0    0    0    0    0\n    0    0    0  335    0    0    0  330  266    0    0    0    0  260\n    0    0    7   68  317    0    0    0    0  124    0    0    0    0\n  306  368  117    0    0    0    0    0    0  110    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0   88    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0   69\n    0    0    0    0    0    0   67  129  129    0    0    0    0    0\n    0    0    0    0    0   65    0    0    0    0    0    0    0    0\n  126    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    7    0    0    0    0    9    0    0    0    0   11\n  449    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0   24    0    0    0    0    0    0    0\n  156    0    0    0  285    0    0    0    0    0   41    0    0    0\n    0    0   47    0  425    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0  197    0    0    0    0    0  143\n    0    0    0  462  213    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0  32   0   0  93  30  29 216   0 152 152\n   0   0  87  24 148  85   0   0  21 270  19 144 143 330   0 141   0  15\n  14 138   0  12 136  73 135   0   8 320  69   0   0   0   0   0 190  64\n  63 250  62   0  60 122   0  58   0 119  55 117   0 115  52   0   0   0\n  49   0   0  46   0 107   0   0   0 104   0   0   0  38 287   0   0   0\n  33 157   0   0   0   0  28   0   0   0 149 211 335   0  83  82  18   0\n 204 141   0   0   0  12 136 135 134 383   0 256  67  67 253   0   0 125\n 249  61   0   0   0 182   0 305   0   0   0  50 112 236   0 171 108 294\n   0  42  41   0   0 226 162   0   0 159 345  94 156  92 216   0   0 275\n   0 148   0   0 270  81  80  79  78 139  76 512 261  10 134  70 194  68\n  67   0 315   0 313   0   0 309 308  57   0 555 116 428   0 238   0 111\n   0 108  45 106   0   0 103 476   0   0 161   0   0 345  93   0   0   0\n  26 213  87  23 147   0   0 143 205   0   0   0  75  11 198   0   0   0\n   0   0 128 314   0   0 186   0   0 307   0   0 366 365 114  50 237  48\n  47 171   0 293 105   0  40 226 100   0   0  96   0 281   0   0   0   0\n   0   0 148   0  21 332   0 205  16   0   0   0 136  10   0   0   0   0\n   0 315  64   0 124  61 247   0   0   0 242   0  52 239   0   0   0   0\n   0   0   0   0   0  39 476 162   0   0   0  32   0   0  29   0   0 275\n   0  85   0  21  82   0  80  16   0  14   0   0   0  72   0   0 193   4\n   3   0  63  62 124   0 121 245 182   0   0 241 177 113   0   0   0   0\n   0   0   0   0   0   0   0   0  99   0 346   0 219   0   0   0   0   0\n   0   0   0   0   0   0   0   0 204   0  14  76   0  73   0   9 133   0\n   0   0   3   0   1   0   0   0   0   0   0 244   0   0   0   0   0   0\n   0   0 172   0   0   0  43   0   0  40   0   0   0 161   0   0   0   0\n   0   0   0   0 152  26   0   0   0   0   0   0   0   0   0 329   0   0\n  13   0  12   0  10   9 133   0 194   0 129   0   0  64   0   0   0   0\n   0   0   0  57   0   0   0   0   0   0 113 300   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0 159  34   0   0   0  30\n   0   0   0   0   0   0  25   0   0   0 147  22   0   0  19   0   0   0\n   0   0   0   0   0 138   0   0   0   0   0   0   0   0   0   0   0   0\n   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0 117   0   0   0   0 365   0   0   0   0   0   0   0   0\n   0   0   0   0  46 296   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0  38 288   0   0   0   0   0   0   0  36   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  95   0 344   0   0   0\n   0   0   0  93  30  30   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0 154   0   0   0   0   0   0   0   0   0   0  92   0\n  29 279   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0   10    0  213   88  260  103    0   88  119  166   41   41  400\n   25   87  275   71  228  149  274  243   40  227  102  540  118  196\n  274  243   86  258  336    0  336  367  226  398    7  382  304  351\n   70  241   85  351  257  444  475  428  413  584  366  366  522  147\n  631  225  428  537  318  787  646  552  677  646  536  489  536  551\n  301  598  551  270  395  582  816  472  394  269  628  488  675  690\n  503  925  503  518  440  565  705  486  627  892  330  783  705  767\n  532  798  751  548  688  641  766  688  703  812  640  531  531  406\n  468  733  546  796  608  483  717  483  482  295  638  732  591  747\n  810  700  716  684  872  684  871  637  605  574  667  417  527  605\n  636  479  385  432  572  463  556  619  853  681  681  899  493  790\n  664  742  383  617  336  632  445  257  476  678  756  538  928  568\n  474  427  677  771  505  645  786  645  785  738  676  722  613  519\n  441  565  799  752  877 1002 1361 1064 1080  923  798  563  688  938\n  953  906  687  593  890  577  640  671  670  498  779  779  685  576\n  825  763  778  715  653  793  871  761  542  917  823 1151 1276  869\n 1072 1025  759  900  759  462  664  664  414  445  539  632  757  538\n  756  615  537  677 1037  818  739  911  723 1176 1066 1379 1128  956\n  753 1049 1330  908 1033 1189 1033  767  673  719 1063  938  734  687\n 1202  811 1233  889  795  685 1029  763 1106  794  809 1168  918 1621\n 1199 1526 1776 1401 1994 1760 1306 1290 1399 1180 1399 1492  867 1007\n  820 1101 1272 1194 1397 1240 1521 1443 1333 1145 1535 1223 1519 1253\n 1159  846  768  940  877  642  845  751  438  422  594  422  327  358\n  421  452   76  217   91  372  184  231  418  355  292  183  151  292\n  338  431 1056 1431 1009 1024 1555 1695 1492 1179 1788 1756 1350 1396\n 1662 1380 1223 1551 1145 1691 1269 1159 1237  737  830  861 1142 1142\n 1094 1500 1219 1234 1171  936  905  717  685  451  716  434  559  574\n  527  417  729  495  619  478  572  571  555  633  570  789  741  913\n 1194  803  521  646  411  770  395  676  738 1409 1549 1143 1939 1501\n 1142  954  672  906 1875 1577 1311 1483 1545 1060  935 1216 1121 1590\n 1574 1792 1479 1010 1104  775  853  821  946  508  632  804  397  287\n  475  521  380  458  629  363  613  831  706  705  736 1158  767 1063\n  641  750  374  530  436  389  310  294  325  496  433  308  276  167\n  307  416  384  446  336  258  382  460  225  287  146  302  458  254\n  191  113  253  174  252  236  189  251  360    0  140  312  296   45\n    0    0    0    0  527   27    0    0  275   25  337  180  430  445\n  335  288  147  381  240  114  145  504  832  737  393  377  377  657\n  235  172  406  202  389  342  201  341  372    0  308  495  167   41\n  260  243  430  852  633  648  788  741  506  411  723  395  301  535\n  143  705 1002 1142 1641 1875 1656 1468 1905 1686 1498 1716 1512 1481\n 1683 1823 1870 1510 1259 1103 1243 1289 1367 1616 1397 1068 1584 1895\n 2004 1957 2488 1956 2080 1674 1439 1048 1281 1265 1077 1530 1967 1529\n 2153 1543 1730 1527 1792 1244  978 1087 1118  711  445  507  491  271\n  537  724  926 1254 2378 1784 1924 2205 1892 1672 1625 1984 1686 1451\n 1029  857 1122  730  746  479  854  400  650  821  898 1210  882 1428\n 1802 2036 1879 1660 1519 1253 1268  955 1095  829  922  984 1483 1951\n 1498  903 1106  746  589  667  557  462  274  711  304  413  600  334\n  349  255  332  347  550  190  346  267  469  391  406  421  233  295\n  294   43  402  198  323  182  118  524  320  273  663  162  396  161\n   98  551  347  159    2  314  157  141    0    0   61   13  122  387\n    0    0    0    0    0    0    0    0    0  225    0  161    0   50\n  175  205    0  282  500  437  296    0    0  200  168  152   11   26\n  322   87  368  133  335  209  271 1083  582  738  628  643  705  751\n 1219 1765 1390 1374  873  653  700  621  261   26   72  259  336    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n  114  145  129    0  269  160  113   50  159   81  190  205  111  142\n  282  204   63  250  328  250  187  265  249  421  170  186  279  154\n  154  185  216  200  137  137  246  184  246  433  339  308  292  307\n  166  244  354  166  166  228  306  243  399   71  133  414  242  242\n  351  242  320  148  288  241  429  335  381  194  178  272  412   84\n  193  177  396  162  177  333   67  349  255  177  239  161  286  286\n  426  160  301  332  316  426  426  285  113  347  207  347   97  332\n  222  206  456  269  206  425  456  363  316  284  206  222  394  238\n  238  144  378  284  316  363  144  285  441  253  207  129  285  410\n  269  301  176  113  363  395  332  348  442  254  223  333  239  395\n  302  458  536  646  724  662  677  490  678  678  615  178  460  413\n  350  257  491  382  367  711  383  524  602  540  633  415  743  837\n  416  291  525  651  651  526  636  542  730  668  887  856  903  887\n  684  357  482  341  139  279   77  139  187  171  234  172  172  282\n  157  267  783  627  440  471  612  534  910  801  910 1067  880  849\n  662  537  803  788  773  664  867  742  712  587  790  556  651  589\n  480  808  777  606  403  575  591  607  811  468  843 1172  813 1173\n  954 1283 1361 1206 1612 1425 1269 1207  849  958  974  803  585  523\n  430  274  399  744 1150  948 1136  855 1043 1137  966 1107  998  921\n  671  578  406  204  220  205   18   81  175   51    5   99  178  100\n    0    0    0   87   72    0   57  105   58   75    0   76    0   46\n  140   78  235  626  533  831  972 1097 1082 1333 1162 1037 1022 1117\n 1133  696  790  619  854 1136  949  528  466  295  280  390  531 1016\n  986 1174  987  613  691  223  271  147  178  320  305  243  275   72\n  104  167   90  106  373  248  108  234  188  189  205  346  394  566\n  411  583  162   22  179  102  243  165  463  870  964  825  747  498\n  452  249  156  532  845  846  612  706  598  364  162  506  944  836\n 1008  790  603  620  402  262  341  435  545  452  421  156  157  220\n  159  269   82  114  271  521  256  460  727  587  587  447  401  292\n   90  231  185  295  218  140  110  189  142   65  159  254  161  146\n  178  256  210  180   55   71  166   73  105   27  106  169  154   61\n  202   62  250  157   17  112  206  129  176  177   52  194  163  117\n  133  118  165   41  182  120  214   90  216   76  342  186  327  281\n  250   95  142  236   34  253  222  363  145  333  193  194  257  242\n  164  258  212  228  119  213  167  121   74  121  216  138  107  155\n  249  390  296  422  422  501  283  361  221  284  190  191  176  238\n  223  395  552  740 1240 1350 1132 1413  820 1179 1133 1086  899  790\n  868  681  869  401  620  573  479  308  589  621  418  465  434  918\n 1153 1325 1216 1044  841  545  404  389  576  373  498  702 1108 1077\n 1171 1015  811  640  452  468  609  343  406  327  249  249  265  202\n  327  359  343  921 1546 1624 1733 1671 1171 1077 1436 1014  795  670\n  514  404  388  247  232  138  247  215  137  465  277  683  995 1261\n 1073 1338 1260 1103  900  681  540  446  367  445  476  569  631  834\n  740  864  692  441  628  440  284  205   95  251  156  265  155  170\n  279  294  293  433  433  526  353  321  336  273  288  397  271  145\n  175  159  174  204  141  171  233  154  231  136  182  150  165  211\n  241  271   98  285  112  205   94  234   76  153  105   42   41  102\n  179   99   67  222  111   48   46   45  153  105   72   71   38  177\n   98  112  204  328   61  122   89  119   55    6  114   97    0  156\n   45  106  135  243  273  286  394  627  484  326  184  197  383  350\n  567  612  532  530  231  291   86    6  113   64  140    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0   53    0    0  131   53  155  108  217  221  201  158\n  326  256  182  396  428  377  514  627  525  428  564  443  697  506\n  677  646  658  697  673  537  611  497  681  732  771  571  567  470\n  685  724  610  427  622  610  661  751  645  653 1090  856  887  703\n  637  723  789  789  879 1082  742  570  640  672  902 1113 1000 1144\n  828  886 1066  851 1003 1351 1769 1331 1229 1136 1440 1350 1237  850\n  659  471  338  268  361  354 1182 1533 1627 1510 1373  974 1279 1201\n  751  634  607  626  704  981  657  876 1606  993 1637 1266 1598 1172\n  864  613  543  695  914  894  523  449  390  484  457  339  355  331\n  284  304   22  241  315  487  335  670  569  373  440  416  303  768\n  740  619  881 1795 1837 1763 1576 1521 1630 2247 1708 1438 1950 1727\n 1130  586 1020 2235 1957 1422  938  852 1277 2019 1425 1136 1640  960\n  687  601  511  530  616  413  475  510  549  495  362  166  295   41\n  244  248  377  517  298  341  494  904  919 1677  954  399  352  161\n  239  165  454  344  930 1399 1883 1367  855 1898 2007 1593 2273 1480\n 1808 1628  937  417  358  311  100  303  186  209  155  194  108   49\n   68    2   13  119   84   52    9  181  177  118   13   52   20  134\n  149    1  125    0  180  188  102    0  141   90  246  289  187  214\n  328  191  136  175   22   15  190  159   89   34  167   84   96   96\n   57    0   18  201   76   37  201   64  103  111  201  181   40   75\n    1  130  157  122  321  622  790  508  418  180  227  180  297  141\n  187    0   27   19   58    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0   78   89\n   54   54  117  183  195  222  261  320  238  230  218  375  296  273\n  300  304  312  371  304  265  316  261  292  355  414  304  265  390\n  406  316  312  328  308  367  312  394  347  535  687  585  417  535\n  558  703  515  632  828  749  351  183  253  496  550  956  765  788\n  757  675  695  570  847 1078 1421 1089  738  476 1035 1074  800  265\n   85   97    0   46   35   54  546 1109 1070  792  847  335  902  601\n  199  191   81  202  191  445   70  386  706  292  695  347  831  406\n  374  167   81  300  507  171  124   38   89  113    0    3   19   31\n   66   23    3   38   89  120   46  144   97   89   27    7   93  277\n  152   62  339 1144  913  667  472  355  413 1038  398  390  945  445\n  273   93  339 1495 1027  448  105  120  659 1050  499  323  628  413\n   73   54  195  280  198   66   54   73   66  116   58    0   11   23\n    3   11  112   30    7   19  187  417  245  534  136   77    0   46\n    0   58   26   85  351  679  878  308  241  706  472  581  788  292\n  569  636  148   58   15   11    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0   41    0  165  225  336\n  513  543  660  570  680  602  559  669  876  727  811  824  957 1061\n 1360 1213  428  852 1366  962  725 1029 1415  676  692  415  317  265\n  486  481 1008 1674 1756 1349 1660 1273 1293  585  507  396  211  388\n 1003  472  306 1401 1595 1804  513  208  135   59   91  100   86  109\n   94  246  219  109  106   55   97  134  139  117  575  267  168   34\n   93    0   42   75   33   23   26   10   20   85    0    0   12   20\n    0    4    0   31    0   38    0    0   51   54   73   17   61   25\n   78    0   21    0   41    0   64   32   32   14   10   80   59    0\n   23    0    0    0   22    6    0   25    6   35    8    0    8   31\n   37   73   26   17   23   54    0   34   63    0    0   19    0    0\n    8    0    0   43    0   94    0   46    0    0    0    0    0   33\n   23   59   74   61   39    0    0   51    0   21    0    0   21    8\n    7    0    0   36   44   61    0   36   18   20   55    8   41   15\n   47   25    0   24    0   60   26   11    0   27   14    5   89    0\n   22   10    0   14   23   11    3   68   58    0   26    2   40   34\n   71   14    0    0    0   13   16    0   52    0   23    0   18    0\n    4    6   45    0    9   76    0   52    9   49   11   20   42   13\n   15    3   38   54    0   24   43   77    4    0   88    0   41    0\n    1    0   32   22   59   15   20   51   30   17    0    0   33    0\n   91    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0  22   0   5  25  71 181\n 264 293 324 289 343 346 331 399 558 604 530 566 748 799 933 795  57 438\n 763 475 232 365 571 233 212  52  36  65  96 103 426 604 562 440 829 485\n 468 152 100  60  14  44 348  54  41 555 502 573  59   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0]\n[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    2    0   28  397  621\n  700  906  956 1013  792  379 1439 1195  368  787 1022   88  126  114\n  114  253   41   15   16    0    0    9   37   10   18   27    0    0\n    9    4   31   18    5    0   27    0    0   48    1    0    0   29\n   12   25   18   15    1   22    6   26   21   11    0    9    0   29\n   23   19   25   29   18    1   30   28    0   15   15    0   26    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0]\n[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   2   0   0   0  11 202 325\n 473 661 556 459 267  75 508 484  54 249 283   0   4   3  13  18   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0]\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef process_file(file, X,Y,event_counter,dno_list = dno_list):\n    \"\"\"\n    takes a file and processes it and adds it to the data set\n    file:path to the file to be processed\n    X:data set to be added to\n    Y:list of labels of data\n    dno_list:list of number of data points in each binning as calculated previously\n    \n    returns updated X and Y and event_counter\n    \"\"\"\n    # fetching data from file and checking if label is considered\n    event_type , event_name = file.split('/')[-1].split('_')\n    loaded_data = np.loadtxt(file, delimiter='\\t').astype(np.int32)\n    if event_type not in event_counter.keys():\n        return X,Y,event_counter\n    \n    y = [0,0,0,0]\n    \n    # setting and updating Y\n    for key,value in event_types.items():\n        if value in file.split('\\\\')[-1]:\n            y[key-1] = 1\n            Y.append(y)\n            event_counter[value] += 1\n    \n    # setting and updating X\n    f = 0\n    for k,dno in zip(X.keys(),dno_list):\n        datlist = []\n        for i in range(chrn):\n            datlist.append(loaded_data[f:f+dno])\n            f = f + int(dno)\n        X[k].append(np.array(datlist).transpose())\n        \n    return X,Y,event_counter","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:43.002622Z","iopub.execute_input":"2024-08-18T09:42:43.003127Z","iopub.status.idle":"2024-08-18T09:42:43.012159Z","shell.execute_reply.started":"2024-08-18T09:42:43.003095Z","shell.execute_reply":"2024-08-18T09:42:43.011282Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# initializing train and test sets\nX_train = {str(key): [] for key in bin_list}\nY_train = []\ntrain_event_counter = {'GRB':0,'TGF':0,'SGR':0,'SFLARE':0}\n\nX_test = {str(key): [] for key in bin_list}\nY_test = []\ntest_event_counter = {'GRB':0,'TGF':0,'SGR':0,'SFLARE':0}\n\n# print(process_file(train_files[1],X_train,Y_train,train_event_counter))\n\n# processing train and test data sets\nfor file in train_files:\n    X_train,Y_train,train_event_counter = process_file(file,X_train,Y_train,train_event_counter)\nprint('training events\\n',train_event_counter)\nprint('total events : ', sum([i for i in train_event_counter.values()]))\nY_train = np.array(Y_train)\nprint('shape of Y_train', Y_train.shape)\nprint('shape of X_train dictionary:')\nfor i in X_train.keys():\n    X_train[i] = np.array(X_train[i])\n#     X_train[i] = np.reshape(X_train[i], X_train[i].shape + tuple([1]))\n    print(type(X_train[i]),X_train[i].shape)\n\nfor file in test_files:\n    X_test,Y_test,test_event_counter = process_file(file,X_test,Y_test,test_event_counter)\nprint('testing events\\n',test_event_counter)\nprint('total events : ', sum([i for i in test_event_counter.values()]))\nY_test = np.array(Y_test)\nprint('shape of Y_test', Y_test.shape)\nprint('shape of X_test dictionary:')\nfor i in X_test.keys():\n    X_test[i] = np.array(X_test[i])\n#     X_test[i] = np.reshape(X_test[i], X_test[i].shape + tuple([1]))\n    print(type(X_test[i]),X_test[i].shape)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:43.013298Z","iopub.execute_input":"2024-08-18T09:42:43.013562Z","iopub.status.idle":"2024-08-18T09:42:55.101966Z","shell.execute_reply.started":"2024-08-18T09:42:43.013533Z","shell.execute_reply":"2024-08-18T09:42:55.101024Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"training events\n {'GRB': 800, 'TGF': 800, 'SGR': 800, 'SFLARE': 800}\ntotal events :  3200\nshape of Y_train (3200, 4)\nshape of X_train dictionary:\n<class 'numpy.ndarray'> (3200, 999, 2)\n<class 'numpy.ndarray'> (3200, 999, 2)\n<class 'numpy.ndarray'> (3200, 999, 2)\n<class 'numpy.ndarray'> (3200, 999, 2)\n<class 'numpy.ndarray'> (3200, 999, 2)\n<class 'numpy.ndarray'> (3200, 999, 2)\ntesting events\n {'GRB': 200, 'TGF': 200, 'SGR': 184, 'SFLARE': 200}\ntotal events :  784\nshape of Y_test (784, 4)\nshape of X_test dictionary:\n<class 'numpy.ndarray'> (784, 999, 2)\n<class 'numpy.ndarray'> (784, 999, 2)\n<class 'numpy.ndarray'> (784, 999, 2)\n<class 'numpy.ndarray'> (784, 999, 2)\n<class 'numpy.ndarray'> (784, 999, 2)\n<class 'numpy.ndarray'> (784, 999, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom scipy.interpolate import interp1d\n\ndef augment_timeseries_data(train_data, train_labels, augmentation_factor=2):\n    \"\"\"\n    Perform data augmentation on a time series training set stored as a dictionary, including labels.\n    \n    Args:\n    train_data (dict): A dictionary containing input data for multiple layers.\n                       Each key corresponds to an input layer, and its value\n                       is a numpy array of shape (samples, time_steps, features).\n    train_labels (numpy.ndarray): Labels corresponding to the training data.\n    augmentation_factor (int): The factor by which to increase the dataset size.\n    \n    Returns:\n    tuple: (augmented_data, augmented_labels)\n    \"\"\"\n    \n    augmented_data = {}\n    augmented_labels = []\n    \n    def time_warp(x, sigma=0.2, knot=4):\n        orig_steps = np.arange(x.shape[1])\n        random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, x.shape[2]))\n        warp_steps = np.linspace(0, x.shape[1]-1., num=knot+2)\n        ret = np.zeros_like(x)\n        for i in range(x.shape[2]):\n            warper = interp1d(warp_steps, random_warps[:, i], kind='linear')\n            warped_steps = warper(orig_steps)\n            for j in range(x.shape[0]):\n                ret[j, :, i] = np.interp(orig_steps, warped_steps, x[j, :, i])\n        return ret\n\n    def magnitude_warp(x, sigma=0.2, knot=4):\n        orig_steps = np.arange(x.shape[1])\n        random_warps = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, x.shape[2]))\n        warp_steps = np.linspace(0, x.shape[1]-1., num=knot+2)\n        ret = np.zeros_like(x)\n        for i in range(x.shape[2]):\n            warper = interp1d(warp_steps, random_warps[:, i], kind='linear')\n            warped_steps = warper(orig_steps)\n            for j in range(x.shape[0]):\n                ret[j, :, i] = x[j, :, i] * warped_steps\n        return ret\n\n    def window_slice(x, reduce_ratio=0.9):\n        target_len = int(reduce_ratio * x.shape[1])\n        if target_len >= x.shape[1]:\n            return x\n        starts = np.random.randint(0, x.shape[1] - target_len, size=(x.shape[0])).astype(int)\n        ends = (target_len + starts).astype(int)\n        \n        ret = np.zeros_like(x)\n        for i, (start, end) in enumerate(zip(starts, ends)):\n            for j in range(x.shape[2]):\n                ret[i, :, j] = np.interp(np.linspace(0, target_len, num=x.shape[1]), \n                                         np.arange(target_len), \n                                         x[i, start:end, j])\n        return ret\n\n    for layer_name in train_data.keys():\n        layer_data = train_data[layer_name]\n        augmented_layer_data = [layer_data]\n        \n        for _ in range(augmentation_factor - 1):\n            aug_data = layer_data.copy()\n            if np.random.rand() > 0.5:\n                aug_data = time_warp(aug_data)\n            if np.random.rand() > 0.5:\n                aug_data = magnitude_warp(aug_data)\n            if np.random.rand() > 0.5:\n                aug_data = window_slice(aug_data)\n            \n            augmented_layer_data.append(aug_data)\n        \n        augmented_data[layer_name] = np.concatenate(augmented_layer_data, axis=0)\n    \n    # Replicate labels for each augmented version\n    augmented_labels = np.tile(train_labels, (augmentation_factor, 1))\n    \n    return augmented_data, augmented_labels\n\nX_train_aug,Y_train_aug = augment_timeseries_data(X_train,Y_train,augmentation_factor=2)\n\nprint(Y_train_aug.shape)       \n\nfor key in X_train_aug.keys():\n    print(f\"Shape of X_train_aug['{key}']: {X_train_aug[key].shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:55.103653Z","iopub.execute_input":"2024-08-18T09:42:55.103942Z","iopub.status.idle":"2024-08-18T09:42:57.248716Z","shell.execute_reply.started":"2024-08-18T09:42:55.103914Z","shell.execute_reply":"2024-08-18T09:42:57.247698Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"(6400, 4)\nShape of X_train_aug['0.004']: (6400, 999, 2)\nShape of X_train_aug['0.016']: (6400, 999, 2)\nShape of X_train_aug['0.064']: (6400, 999, 2)\nShape of X_train_aug['0.256']: (6400, 999, 2)\nShape of X_train_aug['1.024']: (6400, 999, 2)\nShape of X_train_aug['4.096']: (6400, 999, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Dense, concatenate, Input, Conv1D, BatchNormalization, Flatten, Dropout, Reshape, Conv2D, MaxPooling1D\nfrom tensorflow.keras.optimizers import schedules, Adam, Adamax\nfrom keras.regularizers import l1_l2\nimport math\n\ndef create_input_layers(data_dict):\n    input_layers = []\n    conv_layers = []\n    layer_count = 0\n    \n    for dataset_name, dataset_list in data_dict.items():\n        input_shape = tuple(dataset_list.shape[1:])\n        input_layer = Input(shape=input_shape, name=str(dataset_name))\n        \n        filter_size = 4\n        kernel_size = 16 \n        \n        conv1 = Conv1D(filters=filter_size, kernel_size=kernel_size,activation='relu')(input_layer)\n        \n        flat = Flatten()(conv1)\n        \n        dense = Dense(512, activation='relu')(flat) \n        \n        d2 = Dropout(0.5)(dense)\n        \n        input_layers.append(input_layer)\n        conv_layers.append(d2)\n        layer_count = layer_count + 1\n        \n    return input_layers, conv_layers, layer_count\n\ndef build_model(data_dict):\n    input_layers, conv_layers, layer_count = create_input_layers(data_dict)\n    \n    merged = concatenate(conv_layers, axis=-1)\n    reshaped = Reshape((len(data_dict), 512, 1))(merged)\n    \n    conv2d = Conv2D(512, (2, 512), activation='relu')(reshaped)\n    flattened = Flatten()(conv2d)\n    dense = Dense(1024, activation='relu')(flattened) \n    drop = Dropout(0.75)(dense)\n    \n    output = Dense(4, activation='softmax')(drop)\n    \n    model = Model(inputs=input_layers, outputs=output)\n    \n    return model\n\n# Build the model\nmodel = build_model(X_train)\n\n# Print model summary\nmodel.summary()\n\nopt = Adam(learning_rate=0.000005,beta_1=0.7)\n\n# Compile the model\nmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2024-08-18T09:42:57.250043Z","iopub.execute_input":"2024-08-18T09:42:57.250431Z","iopub.status.idle":"2024-08-18T09:43:09.348764Z","shell.execute_reply.started":"2024-08-18T09:42:57.250403Z","shell.execute_reply":"2024-08-18T09:43:09.348063Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"2024-08-18 09:42:58.891129: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-18 09:42:58.891229: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-18 09:42:59.011474: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n\n 0.004 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n 0.016 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n 0.064 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n 0.256 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n 1.024 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n 4.096 (\u001b[38;5;33mInputLayer\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m999\u001b[0m, \u001b[38;5;34m2\u001b[0m)              \u001b[38;5;34m0\u001b[0m  -                 \n\n conv1d (\u001b[38;5;33mConv1D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  0.004[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  0.016[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  0.064[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  0.256[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  1.024[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984\u001b[0m, \u001b[38;5;34m4\u001b[0m)            \u001b[38;5;34m132\u001b[0m  4.096[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n flatten (\u001b[38;5;33mFlatten\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n flatten_5 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3936\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    \n\n dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dense_3 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dense_4 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dense_5 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)         \u001b[38;5;34m2,015,744\u001b[0m  flatten_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n\n dropout_1 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_2 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_3 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_4 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dropout_5 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)                \u001b[38;5;34m0\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    \n (\u001b[38;5;33mConcatenate\u001b[0m)                                       dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                                                     dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                                                     dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                                                     dropout_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  \n                                                     dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n reshape (\u001b[38;5;33mReshape\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m512\u001b[0m, \u001b[38;5;34m1\u001b[0m)           \u001b[38;5;34m0\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n\n conv2d (\u001b[38;5;33mConv2D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m512\u001b[0m)     \u001b[38;5;34m524,800\u001b[0m  reshape[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n flatten_6 (\u001b[38;5;33mFlatten\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2560\u001b[0m)                \u001b[38;5;34m0\u001b[0m  conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n\n dense_6 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)        \u001b[38;5;34m2,622,464\u001b[0m  flatten_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n dropout_6 (\u001b[38;5;33mDropout\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                \u001b[38;5;34m0\u001b[0m  dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n\n dense_7 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)               \u001b[38;5;34m4,100\u001b[0m  dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n\n 0.004 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n 0.016 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n 0.064 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n 0.256 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n 1.024 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n 4.096 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">999</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n\n conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  0.004[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  0.016[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  0.064[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  0.256[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  1.024[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">132</span>  4.096[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3936</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    \n\n dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,015,744</span>  flatten_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n\n dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    \n (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                                                     dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                                                     dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                                                     dropout_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  \n                                                     dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n reshape (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n\n conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span>  reshape[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n flatten_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2560</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n\n dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">2,622,464</span>  flatten_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n\n dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">4,100</span>  dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m15,246,620\u001b[0m (58.16 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,246,620</span> (58.16 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m15,246,620\u001b[0m (58.16 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">15,246,620</span> (58.16 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\n\nfrom tensorflow.keras.callbacks import Callback\n\nclass CustomEarlyStopping(Callback):\n    def __init__(self, threshold=0.9, patience=2):\n        super(CustomEarlyStopping, self).__init__()\n        self.threshold = threshold\n        self.patience = patience\n        self.wait = 0\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get('val_accuracy')\n        if current is None:\n            return\n\n        if current >= self.threshold:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.model.stop_training = True\n        else:\n            self.wait = 0\n\ncustom_early_stopping = CustomEarlyStopping()\n\n# Define early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=1000)\n\n# Train the model\nhistory = model.fit(X_train, Y_train, epochs=2000, batch_size=128*2*2, validation_split=0.2, callbacks=[custom_early_stopping,early_stopping])\n\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-18T09:43:09.349878Z","iopub.execute_input":"2024-08-18T09:43:09.350190Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/2000\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1723974207.868205     108 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1723974207.891520     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1723974207.892294     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\nW0000 00:00:1723974207.893336     108 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step - accuracy: 0.2369 - loss: 210.5482","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1723974209.882953     109 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 603ms/step - accuracy: 0.2441 - loss: 195.0062 - val_accuracy: 0.3984 - val_loss: 4.0003\nEpoch 2/2000\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1723974210.582723     107 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.3377 - loss: 78.6557 - val_accuracy: 0.4859 - val_loss: 2.3436\nEpoch 3/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.3719 - loss: 91.9718 - val_accuracy: 0.5109 - val_loss: 2.6997\nEpoch 4/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.3754 - loss: 55.4692 - val_accuracy: 0.5203 - val_loss: 2.9795\nEpoch 5/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.4087 - loss: 21.5892 - val_accuracy: 0.5266 - val_loss: 3.1695\nEpoch 6/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.4214 - loss: 18.1148 - val_accuracy: 0.5328 - val_loss: 3.2672\nEpoch 7/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4407 - loss: 38.9948 - val_accuracy: 0.5375 - val_loss: 3.3042\nEpoch 8/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4443 - loss: 97.8573 - val_accuracy: 0.5406 - val_loss: 3.3783\nEpoch 9/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4447 - loss: 141.4482 - val_accuracy: 0.5422 - val_loss: 3.3999\nEpoch 10/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.4666 - loss: 11.0004 - val_accuracy: 0.5406 - val_loss: 3.3953\nEpoch 11/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4788 - loss: 9.1289 - val_accuracy: 0.5422 - val_loss: 3.3246\nEpoch 12/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.4686 - loss: 28.1754 - val_accuracy: 0.5422 - val_loss: 3.2309\nEpoch 13/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4878 - loss: 56.5919 - val_accuracy: 0.5422 - val_loss: 3.2516\nEpoch 14/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4984 - loss: 82.6421 - val_accuracy: 0.5437 - val_loss: 3.3353\nEpoch 15/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.4974 - loss: 9.3569 - val_accuracy: 0.5437 - val_loss: 3.2794\nEpoch 16/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5060 - loss: 8.9545 - val_accuracy: 0.5453 - val_loss: 3.2208\nEpoch 17/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5002 - loss: 15.6763 - val_accuracy: 0.5453 - val_loss: 3.2187\nEpoch 18/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5022 - loss: 6.8126 - val_accuracy: 0.5469 - val_loss: 3.1638\nEpoch 19/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5073 - loss: 7.5195 - val_accuracy: 0.5484 - val_loss: 3.0657\nEpoch 20/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5217 - loss: 6.0736 - val_accuracy: 0.5547 - val_loss: 3.0055\nEpoch 21/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5182 - loss: 7.8482 - val_accuracy: 0.5547 - val_loss: 2.9343\nEpoch 22/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5257 - loss: 6.3114 - val_accuracy: 0.5594 - val_loss: 2.8682\nEpoch 23/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5071 - loss: 8.4701 - val_accuracy: 0.5656 - val_loss: 2.8074\nEpoch 24/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5457 - loss: 93.9308 - val_accuracy: 0.5625 - val_loss: 2.7602\nEpoch 25/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5276 - loss: 6.5644 - val_accuracy: 0.5625 - val_loss: 2.7465\nEpoch 26/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5147 - loss: 6.6570 - val_accuracy: 0.5625 - val_loss: 2.7212\nEpoch 27/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5398 - loss: 9.4523 - val_accuracy: 0.5656 - val_loss: 2.6484\nEpoch 28/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5225 - loss: 6.2443 - val_accuracy: 0.5719 - val_loss: 2.5858\nEpoch 29/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5431 - loss: 6.3729 - val_accuracy: 0.5750 - val_loss: 2.5424\nEpoch 30/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5344 - loss: 5.8875 - val_accuracy: 0.5828 - val_loss: 2.5197\nEpoch 31/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5406 - loss: 7.9441 - val_accuracy: 0.5906 - val_loss: 2.4958\nEpoch 32/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5503 - loss: 4.8044 - val_accuracy: 0.5938 - val_loss: 2.4791\nEpoch 33/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5361 - loss: 6.5945 - val_accuracy: 0.5938 - val_loss: 2.4741\nEpoch 34/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5472 - loss: 4.9035 - val_accuracy: 0.6031 - val_loss: 2.4427\nEpoch 35/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5668 - loss: 7.2783 - val_accuracy: 0.6047 - val_loss: 2.4703\nEpoch 36/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5430 - loss: 5.4068 - val_accuracy: 0.6062 - val_loss: 2.4570\nEpoch 37/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.5553 - loss: 5.2602 - val_accuracy: 0.6109 - val_loss: 2.4545\nEpoch 38/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5509 - loss: 4.4090 - val_accuracy: 0.6203 - val_loss: 2.4191\nEpoch 39/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5524 - loss: 4.6399 - val_accuracy: 0.6219 - val_loss: 2.3733\nEpoch 40/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5686 - loss: 18.4238 - val_accuracy: 0.6266 - val_loss: 2.3481\nEpoch 41/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5619 - loss: 32.7611 - val_accuracy: 0.6234 - val_loss: 2.3740\nEpoch 42/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5632 - loss: 4.8495 - val_accuracy: 0.6266 - val_loss: 2.3699\nEpoch 43/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.5618 - loss: 7.1618 - val_accuracy: 0.6266 - val_loss: 2.3707\nEpoch 44/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5803 - loss: 3.8508 - val_accuracy: 0.6266 - val_loss: 2.3543\nEpoch 45/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5592 - loss: 5.2393 - val_accuracy: 0.6328 - val_loss: 2.3605\nEpoch 46/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5597 - loss: 3.7296 - val_accuracy: 0.6344 - val_loss: 2.3571\nEpoch 47/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5719 - loss: 5.4738 - val_accuracy: 0.6328 - val_loss: 2.3694\nEpoch 48/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5784 - loss: 4.3635 - val_accuracy: 0.6328 - val_loss: 2.3599\nEpoch 49/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5764 - loss: 4.6886 - val_accuracy: 0.6328 - val_loss: 2.3428\nEpoch 50/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5745 - loss: 13.7030 - val_accuracy: 0.6359 - val_loss: 2.3366\nEpoch 51/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5671 - loss: 3.5654 - val_accuracy: 0.6391 - val_loss: 2.3224\nEpoch 52/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5648 - loss: 5.3786 - val_accuracy: 0.6391 - val_loss: 2.3125\nEpoch 53/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5899 - loss: 4.5325 - val_accuracy: 0.6422 - val_loss: 2.3204\nEpoch 54/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.5966 - loss: 5.4112 - val_accuracy: 0.6438 - val_loss: 2.2959\nEpoch 55/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5776 - loss: 4.1456 - val_accuracy: 0.6469 - val_loss: 2.2710\nEpoch 56/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5712 - loss: 4.5133 - val_accuracy: 0.6453 - val_loss: 2.2853\nEpoch 57/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5802 - loss: 4.2049 - val_accuracy: 0.6453 - val_loss: 2.2763\nEpoch 58/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5926 - loss: 4.6032 - val_accuracy: 0.6453 - val_loss: 2.2655\nEpoch 59/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5917 - loss: 2.9891 - val_accuracy: 0.6438 - val_loss: 2.2662\nEpoch 60/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5888 - loss: 6.3486 - val_accuracy: 0.6453 - val_loss: 2.2623\nEpoch 61/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5901 - loss: 3.8337 - val_accuracy: 0.6453 - val_loss: 2.2497\nEpoch 62/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5867 - loss: 3.0697 - val_accuracy: 0.6469 - val_loss: 2.2457\nEpoch 63/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5875 - loss: 4.1884 - val_accuracy: 0.6469 - val_loss: 2.2375\nEpoch 64/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5865 - loss: 4.1519 - val_accuracy: 0.6469 - val_loss: 2.2556\nEpoch 65/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6061 - loss: 6.2383 - val_accuracy: 0.6500 - val_loss: 2.2670\nEpoch 66/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.6077 - loss: 5.0318 - val_accuracy: 0.6562 - val_loss: 2.2655\nEpoch 67/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5814 - loss: 3.5606 - val_accuracy: 0.6578 - val_loss: 2.2780\nEpoch 68/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6031 - loss: 3.1289 - val_accuracy: 0.6578 - val_loss: 2.2876\nEpoch 69/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6048 - loss: 3.8238 - val_accuracy: 0.6578 - val_loss: 2.2817\nEpoch 70/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6011 - loss: 4.0694 - val_accuracy: 0.6594 - val_loss: 2.2720\nEpoch 71/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6082 - loss: 2.7456 - val_accuracy: 0.6641 - val_loss: 2.2407\nEpoch 72/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.5903 - loss: 5.1992 - val_accuracy: 0.6656 - val_loss: 2.2325\nEpoch 73/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.5919 - loss: 4.3860 - val_accuracy: 0.6656 - val_loss: 2.2468\nEpoch 74/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6040 - loss: 2.9130 - val_accuracy: 0.6656 - val_loss: 2.2593\nEpoch 75/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.6049 - loss: 3.1940 - val_accuracy: 0.6687 - val_loss: 2.2496\nEpoch 76/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6092 - loss: 3.8460 - val_accuracy: 0.6687 - val_loss: 2.2472\nEpoch 77/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6030 - loss: 3.9082 - val_accuracy: 0.6719 - val_loss: 2.2410\nEpoch 78/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6036 - loss: 3.5384 - val_accuracy: 0.6687 - val_loss: 2.2546\nEpoch 79/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6287 - loss: 2.8835 - val_accuracy: 0.6703 - val_loss: 2.2495\nEpoch 80/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6108 - loss: 3.0187 - val_accuracy: 0.6766 - val_loss: 2.2374\nEpoch 81/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6125 - loss: 3.0100 - val_accuracy: 0.6797 - val_loss: 2.2176\nEpoch 82/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6119 - loss: 2.6144 - val_accuracy: 0.6812 - val_loss: 2.2037\nEpoch 83/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6307 - loss: 2.7443 - val_accuracy: 0.6812 - val_loss: 2.2428\nEpoch 84/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6218 - loss: 2.7205 - val_accuracy: 0.6781 - val_loss: 2.2625\nEpoch 85/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6113 - loss: 3.2765 - val_accuracy: 0.6812 - val_loss: 2.2575\nEpoch 86/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6230 - loss: 3.5763 - val_accuracy: 0.6812 - val_loss: 2.2579\nEpoch 87/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6304 - loss: 2.9710 - val_accuracy: 0.6828 - val_loss: 2.2530\nEpoch 88/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6290 - loss: 4.6197 - val_accuracy: 0.6859 - val_loss: 2.2423\nEpoch 89/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6186 - loss: 3.0205 - val_accuracy: 0.6875 - val_loss: 2.2312\nEpoch 90/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6312 - loss: 2.8832 - val_accuracy: 0.6891 - val_loss: 2.2176\nEpoch 91/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6457 - loss: 3.1261 - val_accuracy: 0.6891 - val_loss: 2.2138\nEpoch 92/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6330 - loss: 2.5290 - val_accuracy: 0.6922 - val_loss: 2.2046\nEpoch 93/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6259 - loss: 2.5233 - val_accuracy: 0.6938 - val_loss: 2.1965\nEpoch 94/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6331 - loss: 2.5678 - val_accuracy: 0.6953 - val_loss: 2.1867\nEpoch 95/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6257 - loss: 3.2097 - val_accuracy: 0.6984 - val_loss: 2.1783\nEpoch 96/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 206ms/step - accuracy: 0.6390 - loss: 2.5825 - val_accuracy: 0.6984 - val_loss: 2.1727\nEpoch 97/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6347 - loss: 2.9692 - val_accuracy: 0.6984 - val_loss: 2.1712\nEpoch 98/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6474 - loss: 2.6620 - val_accuracy: 0.6984 - val_loss: 2.1704\nEpoch 99/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6339 - loss: 2.8614 - val_accuracy: 0.6984 - val_loss: 2.1795\nEpoch 100/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6490 - loss: 2.5795 - val_accuracy: 0.7000 - val_loss: 2.1691\nEpoch 101/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6379 - loss: 61.3392 - val_accuracy: 0.7047 - val_loss: 2.1705\nEpoch 102/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6432 - loss: 3.5130 - val_accuracy: 0.7047 - val_loss: 2.1684\nEpoch 103/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6585 - loss: 2.7681 - val_accuracy: 0.7109 - val_loss: 2.1567\nEpoch 104/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6447 - loss: 2.4619 - val_accuracy: 0.7094 - val_loss: 2.1536\nEpoch 105/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6560 - loss: 2.7020 - val_accuracy: 0.7078 - val_loss: 2.1528\nEpoch 106/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6351 - loss: 2.8316 - val_accuracy: 0.7078 - val_loss: 2.1468\nEpoch 107/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6449 - loss: 3.5449 - val_accuracy: 0.7078 - val_loss: 2.1329\nEpoch 108/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 213ms/step - accuracy: 0.6482 - loss: 2.8439 - val_accuracy: 0.7125 - val_loss: 2.1251\nEpoch 109/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6391 - loss: 2.3422 - val_accuracy: 0.7156 - val_loss: 2.1145\nEpoch 110/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6563 - loss: 2.0933 - val_accuracy: 0.7156 - val_loss: 2.1030\nEpoch 111/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6557 - loss: 12.7126 - val_accuracy: 0.7141 - val_loss: 2.1070\nEpoch 112/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6415 - loss: 2.6395 - val_accuracy: 0.7156 - val_loss: 2.1127\nEpoch 113/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6488 - loss: 2.4248 - val_accuracy: 0.7141 - val_loss: 2.1017\nEpoch 114/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 0.6533 - loss: 2.1380 - val_accuracy: 0.7141 - val_loss: 2.0881\nEpoch 115/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.6473 - loss: 2.6941 - val_accuracy: 0.7203 - val_loss: 2.0836\nEpoch 116/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6681 - loss: 2.3657 - val_accuracy: 0.7203 - val_loss: 2.0803\nEpoch 117/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6474 - loss: 3.1038 - val_accuracy: 0.7203 - val_loss: 2.0995\nEpoch 118/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6576 - loss: 2.8240 - val_accuracy: 0.7203 - val_loss: 2.1003\nEpoch 119/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6622 - loss: 2.5763 - val_accuracy: 0.7250 - val_loss: 2.0831\nEpoch 120/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6505 - loss: 2.1541 - val_accuracy: 0.7281 - val_loss: 2.0720\nEpoch 121/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6639 - loss: 2.2363 - val_accuracy: 0.7281 - val_loss: 2.0677\nEpoch 122/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6655 - loss: 2.5494 - val_accuracy: 0.7266 - val_loss: 2.0726\nEpoch 123/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6754 - loss: 3.5173 - val_accuracy: 0.7266 - val_loss: 2.0776\nEpoch 124/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6498 - loss: 2.3269 - val_accuracy: 0.7250 - val_loss: 2.0796\nEpoch 125/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6591 - loss: 2.1741 - val_accuracy: 0.7297 - val_loss: 2.0639\nEpoch 126/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6580 - loss: 2.4975 - val_accuracy: 0.7297 - val_loss: 2.0558\nEpoch 127/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6595 - loss: 2.9269 - val_accuracy: 0.7312 - val_loss: 2.0502\nEpoch 128/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6658 - loss: 2.6860 - val_accuracy: 0.7328 - val_loss: 2.0565\nEpoch 129/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6503 - loss: 50.6064 - val_accuracy: 0.7312 - val_loss: 2.0829\nEpoch 130/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6562 - loss: 2.3090 - val_accuracy: 0.7297 - val_loss: 2.0789\nEpoch 131/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6584 - loss: 2.3338 - val_accuracy: 0.7297 - val_loss: 2.0888\nEpoch 132/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6642 - loss: 2.0471 - val_accuracy: 0.7297 - val_loss: 2.0842\nEpoch 133/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6617 - loss: 2.2527 - val_accuracy: 0.7297 - val_loss: 2.0693\nEpoch 134/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6503 - loss: 2.4300 - val_accuracy: 0.7312 - val_loss: 2.0517\nEpoch 135/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6663 - loss: 1.8728 - val_accuracy: 0.7328 - val_loss: 2.0458\nEpoch 136/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6525 - loss: 2.3457 - val_accuracy: 0.7344 - val_loss: 2.0493\nEpoch 137/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6708 - loss: 2.3732 - val_accuracy: 0.7328 - val_loss: 2.0549\nEpoch 138/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6676 - loss: 2.5607 - val_accuracy: 0.7344 - val_loss: 2.0411\nEpoch 139/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6752 - loss: 2.2658 - val_accuracy: 0.7359 - val_loss: 2.0418\nEpoch 140/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6641 - loss: 1.9243 - val_accuracy: 0.7359 - val_loss: 2.0412\nEpoch 141/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6764 - loss: 2.7205 - val_accuracy: 0.7391 - val_loss: 2.0386\nEpoch 142/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6641 - loss: 2.0978 - val_accuracy: 0.7391 - val_loss: 2.0334\nEpoch 143/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.6842 - loss: 1.7664 - val_accuracy: 0.7391 - val_loss: 2.0336\nEpoch 144/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6683 - loss: 2.0166 - val_accuracy: 0.7375 - val_loss: 2.0467\nEpoch 145/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.6692 - loss: 2.1332 - val_accuracy: 0.7375 - val_loss: 2.0475\nEpoch 146/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6882 - loss: 1.8466 - val_accuracy: 0.7469 - val_loss: 2.0392\nEpoch 147/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6801 - loss: 109.1553 - val_accuracy: 0.7453 - val_loss: 2.0238\nEpoch 148/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6795 - loss: 2.0368 - val_accuracy: 0.7453 - val_loss: 2.0235\nEpoch 149/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6842 - loss: 1.9169 - val_accuracy: 0.7453 - val_loss: 2.0224\nEpoch 150/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6732 - loss: 2.2674 - val_accuracy: 0.7437 - val_loss: 2.0194\nEpoch 151/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6752 - loss: 2.1199 - val_accuracy: 0.7469 - val_loss: 2.0140\nEpoch 152/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6817 - loss: 1.9069 - val_accuracy: 0.7469 - val_loss: 2.0079\nEpoch 153/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6749 - loss: 3.5721 - val_accuracy: 0.7500 - val_loss: 1.9917\nEpoch 154/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6936 - loss: 2.0553 - val_accuracy: 0.7516 - val_loss: 1.9925\nEpoch 155/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6829 - loss: 2.1789 - val_accuracy: 0.7516 - val_loss: 2.0084\nEpoch 156/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6938 - loss: 1.8790 - val_accuracy: 0.7531 - val_loss: 2.0126\nEpoch 157/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.6813 - loss: 1.9195 - val_accuracy: 0.7547 - val_loss: 2.0115\nEpoch 158/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6828 - loss: 2.3048 - val_accuracy: 0.7547 - val_loss: 2.0043\nEpoch 159/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7019 - loss: 2.0459 - val_accuracy: 0.7547 - val_loss: 1.9988\nEpoch 160/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6730 - loss: 1.7793 - val_accuracy: 0.7563 - val_loss: 1.9953\nEpoch 161/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7054 - loss: 2.0052 - val_accuracy: 0.7563 - val_loss: 1.9979\nEpoch 162/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7099 - loss: 1.5939 - val_accuracy: 0.7578 - val_loss: 1.9978\nEpoch 163/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7033 - loss: 1.9911 - val_accuracy: 0.7578 - val_loss: 1.9998\nEpoch 164/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6991 - loss: 2.0447 - val_accuracy: 0.7578 - val_loss: 2.0074\nEpoch 165/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6934 - loss: 1.5754 - val_accuracy: 0.7578 - val_loss: 2.0140\nEpoch 166/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7068 - loss: 1.9082 - val_accuracy: 0.7578 - val_loss: 2.0033\nEpoch 167/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6885 - loss: 1.9744 - val_accuracy: 0.7594 - val_loss: 1.9970\nEpoch 168/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7227 - loss: 1.7510 - val_accuracy: 0.7609 - val_loss: 1.9958\nEpoch 169/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6858 - loss: 2.0896 - val_accuracy: 0.7625 - val_loss: 2.0120\nEpoch 170/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 0.7080 - loss: 1.7704 - val_accuracy: 0.7625 - val_loss: 2.0319\nEpoch 171/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.6996 - loss: 1.9379 - val_accuracy: 0.7625 - val_loss: 2.0293\nEpoch 172/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7016 - loss: 1.9577 - val_accuracy: 0.7625 - val_loss: 2.0222\nEpoch 173/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6945 - loss: 1.6834 - val_accuracy: 0.7656 - val_loss: 2.0150\nEpoch 174/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7077 - loss: 2.0544 - val_accuracy: 0.7641 - val_loss: 2.0313\nEpoch 175/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7038 - loss: 1.6029 - val_accuracy: 0.7641 - val_loss: 2.0301\nEpoch 176/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7099 - loss: 1.7799 - val_accuracy: 0.7656 - val_loss: 2.0258\nEpoch 177/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7153 - loss: 1.8895 - val_accuracy: 0.7672 - val_loss: 2.0182\nEpoch 178/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7089 - loss: 1.9962 - val_accuracy: 0.7688 - val_loss: 2.0097\nEpoch 179/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6953 - loss: 1.7601 - val_accuracy: 0.7688 - val_loss: 2.0018\nEpoch 180/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6964 - loss: 2.0211 - val_accuracy: 0.7703 - val_loss: 2.0145\nEpoch 181/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7068 - loss: 1.6598 - val_accuracy: 0.7703 - val_loss: 2.0249\nEpoch 182/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7206 - loss: 1.6026 - val_accuracy: 0.7703 - val_loss: 2.0218\nEpoch 183/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7020 - loss: 1.9003 - val_accuracy: 0.7703 - val_loss: 2.0145\nEpoch 184/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7124 - loss: 1.4793 - val_accuracy: 0.7719 - val_loss: 2.0092\nEpoch 185/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7151 - loss: 67.4370 - val_accuracy: 0.7734 - val_loss: 1.9982\nEpoch 186/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7007 - loss: 2.0566 - val_accuracy: 0.7719 - val_loss: 2.0014\nEpoch 187/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7197 - loss: 2.2758 - val_accuracy: 0.7719 - val_loss: 2.0094\nEpoch 188/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.6971 - loss: 1.8970 - val_accuracy: 0.7719 - val_loss: 2.0160\nEpoch 189/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7034 - loss: 1.9187 - val_accuracy: 0.7719 - val_loss: 2.0214\nEpoch 190/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7137 - loss: 1.7176 - val_accuracy: 0.7734 - val_loss: 2.0195\nEpoch 191/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7180 - loss: 2.0522 - val_accuracy: 0.7734 - val_loss: 2.0090\nEpoch 192/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7256 - loss: 1.5022 - val_accuracy: 0.7750 - val_loss: 1.9994\nEpoch 193/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7185 - loss: 1.4488 - val_accuracy: 0.7781 - val_loss: 1.9927\nEpoch 194/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7141 - loss: 1.3588 - val_accuracy: 0.7781 - val_loss: 1.9896\nEpoch 195/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7081 - loss: 1.7343 - val_accuracy: 0.7781 - val_loss: 1.9863\nEpoch 196/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7182 - loss: 1.5542 - val_accuracy: 0.7781 - val_loss: 1.9815\nEpoch 197/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7218 - loss: 1.8220 - val_accuracy: 0.7781 - val_loss: 1.9823\nEpoch 198/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7255 - loss: 1.8100 - val_accuracy: 0.7750 - val_loss: 1.9803\nEpoch 199/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7212 - loss: 3.8906 - val_accuracy: 0.7797 - val_loss: 1.9752\nEpoch 200/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7167 - loss: 1.5873 - val_accuracy: 0.7828 - val_loss: 1.9699\nEpoch 201/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7143 - loss: 1.7125 - val_accuracy: 0.7844 - val_loss: 1.9731\nEpoch 202/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7095 - loss: 1.4002 - val_accuracy: 0.7828 - val_loss: 1.9712\nEpoch 203/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7182 - loss: 1.9521 - val_accuracy: 0.7859 - val_loss: 1.9686\nEpoch 204/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7218 - loss: 2.2648 - val_accuracy: 0.7875 - val_loss: 1.9660\nEpoch 205/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 207ms/step - accuracy: 0.7264 - loss: 1.5200 - val_accuracy: 0.7859 - val_loss: 1.9629\nEpoch 206/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7313 - loss: 1.6031 - val_accuracy: 0.7844 - val_loss: 1.9561\nEpoch 207/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7187 - loss: 1.4938 - val_accuracy: 0.7844 - val_loss: 1.9529\nEpoch 208/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7391 - loss: 1.2328 - val_accuracy: 0.7844 - val_loss: 1.9514\nEpoch 209/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7183 - loss: 1.7799 - val_accuracy: 0.7844 - val_loss: 1.9483\nEpoch 210/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7288 - loss: 1.6363 - val_accuracy: 0.7891 - val_loss: 1.9546\nEpoch 211/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7346 - loss: 1.3811 - val_accuracy: 0.7953 - val_loss: 1.9646\nEpoch 212/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7282 - loss: 1.5802 - val_accuracy: 0.7953 - val_loss: 1.9741\nEpoch 213/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7323 - loss: 13.5781 - val_accuracy: 0.7906 - val_loss: 1.9848\nEpoch 214/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7253 - loss: 17.1734 - val_accuracy: 0.7906 - val_loss: 2.0095\nEpoch 215/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7095 - loss: 1.6821 - val_accuracy: 0.7922 - val_loss: 2.0142\nEpoch 216/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7085 - loss: 1.8498 - val_accuracy: 0.7922 - val_loss: 2.0136\nEpoch 217/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7311 - loss: 2.4660 - val_accuracy: 0.7922 - val_loss: 2.0101\nEpoch 218/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7301 - loss: 1.3582 - val_accuracy: 0.7922 - val_loss: 2.0095\nEpoch 219/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7277 - loss: 1.4957 - val_accuracy: 0.7922 - val_loss: 2.0112\nEpoch 220/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7386 - loss: 1.3868 - val_accuracy: 0.7969 - val_loss: 2.0102\nEpoch 221/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7192 - loss: 1.6575 - val_accuracy: 0.7969 - val_loss: 2.0102\nEpoch 222/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7401 - loss: 1.3337 - val_accuracy: 0.7953 - val_loss: 2.0093\nEpoch 223/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7400 - loss: 1.2402 - val_accuracy: 0.7953 - val_loss: 2.0079\nEpoch 224/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7314 - loss: 1.3543 - val_accuracy: 0.7969 - val_loss: 2.0030\nEpoch 225/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7261 - loss: 1.6236 - val_accuracy: 0.7969 - val_loss: 2.0022\nEpoch 226/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7402 - loss: 1.3035 - val_accuracy: 0.7969 - val_loss: 2.0012\nEpoch 227/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7310 - loss: 1.4284 - val_accuracy: 0.7984 - val_loss: 2.0002\nEpoch 228/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7347 - loss: 1.6201 - val_accuracy: 0.8016 - val_loss: 2.0009\nEpoch 229/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7417 - loss: 1.3713 - val_accuracy: 0.8047 - val_loss: 2.0004\nEpoch 230/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7458 - loss: 1.2774 - val_accuracy: 0.8047 - val_loss: 1.9974\nEpoch 231/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7490 - loss: 1.3637 - val_accuracy: 0.8047 - val_loss: 1.9955\nEpoch 232/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7564 - loss: 1.2483 - val_accuracy: 0.8047 - val_loss: 1.9991\nEpoch 233/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7287 - loss: 1.3899 - val_accuracy: 0.8062 - val_loss: 2.0003\nEpoch 234/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7531 - loss: 1.2792 - val_accuracy: 0.8078 - val_loss: 2.0000\nEpoch 235/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7466 - loss: 1.2643 - val_accuracy: 0.8094 - val_loss: 2.0005\nEpoch 236/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7424 - loss: 2.1407 - val_accuracy: 0.8094 - val_loss: 1.9970\nEpoch 237/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7517 - loss: 1.3037 - val_accuracy: 0.8109 - val_loss: 1.9915\nEpoch 238/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7424 - loss: 1.3592 - val_accuracy: 0.8109 - val_loss: 1.9934\nEpoch 239/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7293 - loss: 1.2636 - val_accuracy: 0.8109 - val_loss: 1.9928\nEpoch 240/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7391 - loss: 2.0385 - val_accuracy: 0.8109 - val_loss: 1.9932\nEpoch 241/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7364 - loss: 1.3284 - val_accuracy: 0.8109 - val_loss: 1.9908\nEpoch 242/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7428 - loss: 1.6680 - val_accuracy: 0.8094 - val_loss: 1.9934\nEpoch 243/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7459 - loss: 1.3199 - val_accuracy: 0.8078 - val_loss: 1.9948\nEpoch 244/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7524 - loss: 1.2026 - val_accuracy: 0.8094 - val_loss: 1.9951\nEpoch 245/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7315 - loss: 1.5979 - val_accuracy: 0.8094 - val_loss: 1.9961\nEpoch 246/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7488 - loss: 1.3671 - val_accuracy: 0.8109 - val_loss: 1.9957\nEpoch 247/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7487 - loss: 1.3056 - val_accuracy: 0.8109 - val_loss: 1.9992\nEpoch 248/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7424 - loss: 1.3781 - val_accuracy: 0.8094 - val_loss: 2.0087\nEpoch 249/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7462 - loss: 1.4106 - val_accuracy: 0.8094 - val_loss: 2.0151\nEpoch 250/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7584 - loss: 2.1575 - val_accuracy: 0.8094 - val_loss: 2.0176\nEpoch 251/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7361 - loss: 1.2373 - val_accuracy: 0.8109 - val_loss: 2.0182\nEpoch 252/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7610 - loss: 1.2529 - val_accuracy: 0.8094 - val_loss: 2.0169\nEpoch 253/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7490 - loss: 1.5604 - val_accuracy: 0.8094 - val_loss: 2.0140\nEpoch 254/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7418 - loss: 1.3212 - val_accuracy: 0.8109 - val_loss: 2.0155\nEpoch 255/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7505 - loss: 1.3748 - val_accuracy: 0.8109 - val_loss: 2.0177\nEpoch 256/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7448 - loss: 2.2527 - val_accuracy: 0.8141 - val_loss: 2.0235\nEpoch 257/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7416 - loss: 1.4472 - val_accuracy: 0.8141 - val_loss: 2.0278\nEpoch 258/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7548 - loss: 1.3751 - val_accuracy: 0.8141 - val_loss: 2.0274\nEpoch 259/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7498 - loss: 1.1757 - val_accuracy: 0.8141 - val_loss: 2.0295\nEpoch 260/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 0.7518 - loss: 1.2367 - val_accuracy: 0.8141 - val_loss: 2.0286\nEpoch 261/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7488 - loss: 1.1684 - val_accuracy: 0.8141 - val_loss: 2.0307\nEpoch 262/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7548 - loss: 1.2666 - val_accuracy: 0.8125 - val_loss: 2.0325\nEpoch 263/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7628 - loss: 1.3686 - val_accuracy: 0.8125 - val_loss: 2.0314\nEpoch 264/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7511 - loss: 1.1722 - val_accuracy: 0.8141 - val_loss: 2.0331\nEpoch 265/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7638 - loss: 1.1897 - val_accuracy: 0.8156 - val_loss: 2.0330\nEpoch 266/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7678 - loss: 1.3917 - val_accuracy: 0.8172 - val_loss: 2.0320\nEpoch 267/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7628 - loss: 1.2235 - val_accuracy: 0.8188 - val_loss: 2.0366\nEpoch 268/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7615 - loss: 1.0712 - val_accuracy: 0.8188 - val_loss: 2.0398\nEpoch 269/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7471 - loss: 1.3628 - val_accuracy: 0.8203 - val_loss: 2.0420\nEpoch 270/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7551 - loss: 1.5987 - val_accuracy: 0.8203 - val_loss: 2.0410\nEpoch 271/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7621 - loss: 1.3016 - val_accuracy: 0.8203 - val_loss: 2.0389\nEpoch 272/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7585 - loss: 1.0957 - val_accuracy: 0.8203 - val_loss: 2.0369\nEpoch 273/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7542 - loss: 1.4703 - val_accuracy: 0.8234 - val_loss: 2.0403\nEpoch 274/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7660 - loss: 1.1268 - val_accuracy: 0.8234 - val_loss: 2.0383\nEpoch 275/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7720 - loss: 1.3298 - val_accuracy: 0.8266 - val_loss: 2.0390\nEpoch 276/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7669 - loss: 1.4447 - val_accuracy: 0.8266 - val_loss: 2.0427\nEpoch 277/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7599 - loss: 1.1777 - val_accuracy: 0.8281 - val_loss: 2.0448\nEpoch 278/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7759 - loss: 1.0989 - val_accuracy: 0.8281 - val_loss: 2.0440\nEpoch 279/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7549 - loss: 1.0023 - val_accuracy: 0.8313 - val_loss: 2.0435\nEpoch 280/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7681 - loss: 1.4527 - val_accuracy: 0.8313 - val_loss: 2.0413\nEpoch 281/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7628 - loss: 1.1211 - val_accuracy: 0.8313 - val_loss: 2.0398\nEpoch 282/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7694 - loss: 1.4388 - val_accuracy: 0.8313 - val_loss: 2.0374\nEpoch 283/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7795 - loss: 1.2943 - val_accuracy: 0.8313 - val_loss: 2.0347\nEpoch 284/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7700 - loss: 1.2872 - val_accuracy: 0.8313 - val_loss: 2.0352\nEpoch 285/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7678 - loss: 1.4890 - val_accuracy: 0.8328 - val_loss: 2.0366\nEpoch 286/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7647 - loss: 1.1576 - val_accuracy: 0.8344 - val_loss: 2.0415\nEpoch 287/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7748 - loss: 1.0997 - val_accuracy: 0.8359 - val_loss: 2.0439\nEpoch 288/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7629 - loss: 1.0730 - val_accuracy: 0.8375 - val_loss: 2.0458\nEpoch 289/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7642 - loss: 1.3524 - val_accuracy: 0.8375 - val_loss: 2.0483\nEpoch 290/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7557 - loss: 1.3339 - val_accuracy: 0.8375 - val_loss: 2.0550\nEpoch 291/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7738 - loss: 1.0127 - val_accuracy: 0.8391 - val_loss: 2.0559\nEpoch 292/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7690 - loss: 1.0956 - val_accuracy: 0.8391 - val_loss: 2.0539\nEpoch 293/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7770 - loss: 1.0080 - val_accuracy: 0.8391 - val_loss: 2.0507\nEpoch 294/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7695 - loss: 1.4320 - val_accuracy: 0.8406 - val_loss: 2.0489\nEpoch 295/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7744 - loss: 1.0180 - val_accuracy: 0.8406 - val_loss: 2.0442\nEpoch 296/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7712 - loss: 1.3497 - val_accuracy: 0.8406 - val_loss: 2.0425\nEpoch 297/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7838 - loss: 1.1778 - val_accuracy: 0.8406 - val_loss: 2.0412\nEpoch 298/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7832 - loss: 1.0842 - val_accuracy: 0.8391 - val_loss: 2.0413\nEpoch 299/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7818 - loss: 1.1810 - val_accuracy: 0.8391 - val_loss: 2.0414\nEpoch 300/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7796 - loss: 1.1959 - val_accuracy: 0.8391 - val_loss: 2.0404\nEpoch 301/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7684 - loss: 1.1554 - val_accuracy: 0.8391 - val_loss: 2.0285\nEpoch 302/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7825 - loss: 1.0841 - val_accuracy: 0.8406 - val_loss: 2.0258\nEpoch 303/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7722 - loss: 1.0976 - val_accuracy: 0.8406 - val_loss: 2.0268\nEpoch 304/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7730 - loss: 1.1405 - val_accuracy: 0.8406 - val_loss: 2.0290\nEpoch 305/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7888 - loss: 0.9672 - val_accuracy: 0.8406 - val_loss: 2.0292\nEpoch 306/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7733 - loss: 1.1457 - val_accuracy: 0.8406 - val_loss: 2.0296\nEpoch 307/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7788 - loss: 1.0155 - val_accuracy: 0.8406 - val_loss: 2.0298\nEpoch 308/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7804 - loss: 1.1768 - val_accuracy: 0.8406 - val_loss: 2.0351\nEpoch 309/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7747 - loss: 1.0956 - val_accuracy: 0.8406 - val_loss: 2.0368\nEpoch 310/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7860 - loss: 0.9616 - val_accuracy: 0.8422 - val_loss: 2.0402\nEpoch 311/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7759 - loss: 1.4530 - val_accuracy: 0.8422 - val_loss: 2.0422\nEpoch 312/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7892 - loss: 1.1441 - val_accuracy: 0.8422 - val_loss: 2.0427\nEpoch 313/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7914 - loss: 1.0606 - val_accuracy: 0.8422 - val_loss: 2.0427\nEpoch 314/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7835 - loss: 1.2352 - val_accuracy: 0.8406 - val_loss: 2.0440\nEpoch 315/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7849 - loss: 0.9620 - val_accuracy: 0.8406 - val_loss: 2.0447\nEpoch 316/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7789 - loss: 1.1303 - val_accuracy: 0.8406 - val_loss: 2.0471\nEpoch 317/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7885 - loss: 1.1846 - val_accuracy: 0.8391 - val_loss: 2.0468\nEpoch 318/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7758 - loss: 1.0659 - val_accuracy: 0.8391 - val_loss: 2.0489\nEpoch 319/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7928 - loss: 2.2932 - val_accuracy: 0.8422 - val_loss: 2.0511\nEpoch 320/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7939 - loss: 1.1993 - val_accuracy: 0.8406 - val_loss: 2.0523\nEpoch 321/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7861 - loss: 0.9516 - val_accuracy: 0.8406 - val_loss: 2.0502\nEpoch 322/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7814 - loss: 1.0887 - val_accuracy: 0.8406 - val_loss: 2.0459\nEpoch 323/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7925 - loss: 1.5707 - val_accuracy: 0.8406 - val_loss: 2.0460\nEpoch 324/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7872 - loss: 0.9817 - val_accuracy: 0.8406 - val_loss: 2.0483\nEpoch 325/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7881 - loss: 1.1473 - val_accuracy: 0.8406 - val_loss: 2.0490\nEpoch 326/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.7911 - loss: 1.1581 - val_accuracy: 0.8406 - val_loss: 2.0525\nEpoch 327/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7941 - loss: 1.2836 - val_accuracy: 0.8422 - val_loss: 2.0554\nEpoch 328/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7913 - loss: 1.0673 - val_accuracy: 0.8406 - val_loss: 2.0590\nEpoch 329/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7737 - loss: 1.1087 - val_accuracy: 0.8422 - val_loss: 2.0599\nEpoch 330/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7927 - loss: 0.8672 - val_accuracy: 0.8422 - val_loss: 2.0608\nEpoch 331/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8002 - loss: 1.0985 - val_accuracy: 0.8422 - val_loss: 2.0649\nEpoch 332/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7900 - loss: 0.9820 - val_accuracy: 0.8438 - val_loss: 2.0677\nEpoch 333/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7931 - loss: 1.1888 - val_accuracy: 0.8453 - val_loss: 2.0679\nEpoch 334/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7841 - loss: 1.6356 - val_accuracy: 0.8438 - val_loss: 2.0662\nEpoch 335/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7897 - loss: 1.0759 - val_accuracy: 0.8422 - val_loss: 2.0692\nEpoch 336/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7897 - loss: 2.4728 - val_accuracy: 0.8406 - val_loss: 2.0773\nEpoch 337/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7903 - loss: 0.9463 - val_accuracy: 0.8406 - val_loss: 2.0813\nEpoch 338/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7863 - loss: 0.9634 - val_accuracy: 0.8422 - val_loss: 2.0828\nEpoch 339/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7905 - loss: 1.0634 - val_accuracy: 0.8422 - val_loss: 2.0839\nEpoch 340/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7873 - loss: 1.0671 - val_accuracy: 0.8422 - val_loss: 2.0851\nEpoch 341/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7829 - loss: 1.1391 - val_accuracy: 0.8422 - val_loss: 2.0851\nEpoch 342/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7906 - loss: 1.1808 - val_accuracy: 0.8422 - val_loss: 2.0769\nEpoch 343/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7946 - loss: 1.2007 - val_accuracy: 0.8422 - val_loss: 2.0737\nEpoch 344/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7913 - loss: 1.0848 - val_accuracy: 0.8438 - val_loss: 2.0760\nEpoch 345/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7887 - loss: 0.9584 - val_accuracy: 0.8453 - val_loss: 2.0777\nEpoch 346/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7965 - loss: 1.0709 - val_accuracy: 0.8438 - val_loss: 2.0802\nEpoch 347/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7998 - loss: 0.9401 - val_accuracy: 0.8438 - val_loss: 2.0856\nEpoch 348/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8023 - loss: 0.9482 - val_accuracy: 0.8438 - val_loss: 2.0901\nEpoch 349/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7953 - loss: 1.2529 - val_accuracy: 0.8438 - val_loss: 2.0934\nEpoch 350/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8002 - loss: 0.9346 - val_accuracy: 0.8438 - val_loss: 2.0923\nEpoch 351/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7966 - loss: 2.0831 - val_accuracy: 0.8469 - val_loss: 2.0907\nEpoch 352/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.7999 - loss: 0.9681 - val_accuracy: 0.8469 - val_loss: 2.0924\nEpoch 353/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8110 - loss: 1.1254 - val_accuracy: 0.8469 - val_loss: 2.0972\nEpoch 354/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7967 - loss: 0.9543 - val_accuracy: 0.8469 - val_loss: 2.0959\nEpoch 355/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8026 - loss: 1.0371 - val_accuracy: 0.8453 - val_loss: 2.0956\nEpoch 356/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7905 - loss: 2.1741 - val_accuracy: 0.8453 - val_loss: 2.0914\nEpoch 357/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7990 - loss: 1.1599 - val_accuracy: 0.8453 - val_loss: 2.0934\nEpoch 358/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7971 - loss: 0.9345 - val_accuracy: 0.8453 - val_loss: 2.0902\nEpoch 359/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8007 - loss: 1.0719 - val_accuracy: 0.8469 - val_loss: 2.0924\nEpoch 360/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8224 - loss: 0.8883 - val_accuracy: 0.8469 - val_loss: 2.0940\nEpoch 361/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8089 - loss: 0.9954 - val_accuracy: 0.8469 - val_loss: 2.0965\nEpoch 362/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8156 - loss: 0.8448 - val_accuracy: 0.8500 - val_loss: 2.0974\nEpoch 363/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8049 - loss: 0.8661 - val_accuracy: 0.8484 - val_loss: 2.0987\nEpoch 364/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.7978 - loss: 1.3031 - val_accuracy: 0.8484 - val_loss: 2.0964\nEpoch 365/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7925 - loss: 0.8905 - val_accuracy: 0.8469 - val_loss: 2.0925\nEpoch 366/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7911 - loss: 1.1863 - val_accuracy: 0.8469 - val_loss: 2.0917\nEpoch 367/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8018 - loss: 0.8949 - val_accuracy: 0.8484 - val_loss: 2.0934\nEpoch 368/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8061 - loss: 0.9592 - val_accuracy: 0.8484 - val_loss: 2.1023\nEpoch 369/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8055 - loss: 0.9301 - val_accuracy: 0.8484 - val_loss: 2.1050\nEpoch 370/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7990 - loss: 0.9721 - val_accuracy: 0.8500 - val_loss: 2.1058\nEpoch 371/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8016 - loss: 1.0713 - val_accuracy: 0.8500 - val_loss: 2.1033\nEpoch 372/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8066 - loss: 0.9110 - val_accuracy: 0.8500 - val_loss: 2.1044\nEpoch 373/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8008 - loss: 0.9642 - val_accuracy: 0.8516 - val_loss: 2.1105\nEpoch 374/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8028 - loss: 0.9725 - val_accuracy: 0.8516 - val_loss: 2.1116\nEpoch 375/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.7971 - loss: 0.9845 - val_accuracy: 0.8500 - val_loss: 2.1116\nEpoch 376/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8001 - loss: 1.0234 - val_accuracy: 0.8516 - val_loss: 2.1106\nEpoch 377/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8078 - loss: 0.9104 - val_accuracy: 0.8469 - val_loss: 2.1058\nEpoch 378/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8075 - loss: 0.8822 - val_accuracy: 0.8484 - val_loss: 2.1063\nEpoch 379/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7975 - loss: 2.7954 - val_accuracy: 0.8500 - val_loss: 2.1091\nEpoch 380/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7989 - loss: 0.8538 - val_accuracy: 0.8500 - val_loss: 2.1201\nEpoch 381/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7975 - loss: 0.9443 - val_accuracy: 0.8500 - val_loss: 2.1219\nEpoch 382/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8035 - loss: 0.8173 - val_accuracy: 0.8484 - val_loss: 2.1234\nEpoch 383/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8071 - loss: 0.9030 - val_accuracy: 0.8484 - val_loss: 2.1281\nEpoch 384/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8132 - loss: 0.9346 - val_accuracy: 0.8484 - val_loss: 2.1377\nEpoch 385/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7988 - loss: 0.9294 - val_accuracy: 0.8500 - val_loss: 2.1444\nEpoch 386/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 0.8056 - loss: 0.8673 - val_accuracy: 0.8469 - val_loss: 2.1434\nEpoch 387/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8157 - loss: 0.8539 - val_accuracy: 0.8469 - val_loss: 2.1419\nEpoch 388/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8126 - loss: 0.8582 - val_accuracy: 0.8484 - val_loss: 2.1373\nEpoch 389/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8073 - loss: 0.9985 - val_accuracy: 0.8484 - val_loss: 2.1330\nEpoch 390/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.7996 - loss: 0.9286 - val_accuracy: 0.8484 - val_loss: 2.1385\nEpoch 391/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8064 - loss: 1.5070 - val_accuracy: 0.8484 - val_loss: 2.1422\nEpoch 392/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8233 - loss: 0.9145 - val_accuracy: 0.8500 - val_loss: 2.1473\nEpoch 393/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8230 - loss: 0.8679 - val_accuracy: 0.8500 - val_loss: 2.1532\nEpoch 394/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8132 - loss: 0.9637 - val_accuracy: 0.8516 - val_loss: 2.1544\nEpoch 395/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8119 - loss: 0.7727 - val_accuracy: 0.8531 - val_loss: 2.1528\nEpoch 396/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8081 - loss: 1.1310 - val_accuracy: 0.8531 - val_loss: 2.1539\nEpoch 397/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8006 - loss: 0.8732 - val_accuracy: 0.8531 - val_loss: 2.1588\nEpoch 398/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8252 - loss: 0.8816 - val_accuracy: 0.8531 - val_loss: 2.1602\nEpoch 399/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8129 - loss: 0.8316 - val_accuracy: 0.8531 - val_loss: 2.1582\nEpoch 400/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.8192 - loss: 0.9622 - val_accuracy: 0.8531 - val_loss: 2.1593\nEpoch 401/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8280 - loss: 0.7246 - val_accuracy: 0.8547 - val_loss: 2.1631\nEpoch 402/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8129 - loss: 0.8846 - val_accuracy: 0.8531 - val_loss: 2.1652\nEpoch 403/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8057 - loss: 0.9121 - val_accuracy: 0.8531 - val_loss: 2.1696\nEpoch 404/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8058 - loss: 0.8365 - val_accuracy: 0.8531 - val_loss: 2.1744\nEpoch 405/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8151 - loss: 0.9373 - val_accuracy: 0.8547 - val_loss: 2.1703\nEpoch 406/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8182 - loss: 0.8656 - val_accuracy: 0.8531 - val_loss: 2.1668\nEpoch 407/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8236 - loss: 0.7919 - val_accuracy: 0.8547 - val_loss: 2.1659\nEpoch 408/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8129 - loss: 0.7891 - val_accuracy: 0.8547 - val_loss: 2.1677\nEpoch 409/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8108 - loss: 0.7446 - val_accuracy: 0.8547 - val_loss: 2.1701\nEpoch 410/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8195 - loss: 1.1596 - val_accuracy: 0.8547 - val_loss: 2.1681\nEpoch 411/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8194 - loss: 0.8414 - val_accuracy: 0.8547 - val_loss: 2.1677\nEpoch 412/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8160 - loss: 0.9328 - val_accuracy: 0.8547 - val_loss: 2.1703\nEpoch 413/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8174 - loss: 0.7181 - val_accuracy: 0.8547 - val_loss: 2.1713\nEpoch 414/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8201 - loss: 0.8469 - val_accuracy: 0.8547 - val_loss: 2.1697\nEpoch 415/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8122 - loss: 0.8147 - val_accuracy: 0.8562 - val_loss: 2.1689\nEpoch 416/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8281 - loss: 0.8810 - val_accuracy: 0.8562 - val_loss: 2.1724\nEpoch 417/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8247 - loss: 0.8321 - val_accuracy: 0.8562 - val_loss: 2.1728\nEpoch 418/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8217 - loss: 0.8569 - val_accuracy: 0.8578 - val_loss: 2.1707\nEpoch 419/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8153 - loss: 0.9069 - val_accuracy: 0.8578 - val_loss: 2.1678\nEpoch 420/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.8184 - loss: 0.8941 - val_accuracy: 0.8578 - val_loss: 2.1660\nEpoch 421/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8279 - loss: 0.7580 - val_accuracy: 0.8594 - val_loss: 2.1719\nEpoch 422/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8195 - loss: 0.9132 - val_accuracy: 0.8594 - val_loss: 2.1767\nEpoch 423/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8226 - loss: 0.9476 - val_accuracy: 0.8594 - val_loss: 2.1795\nEpoch 424/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8104 - loss: 0.9849 - val_accuracy: 0.8594 - val_loss: 2.1775\nEpoch 425/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8213 - loss: 0.8433 - val_accuracy: 0.8594 - val_loss: 2.1753\nEpoch 426/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8142 - loss: 0.8891 - val_accuracy: 0.8594 - val_loss: 2.1761\nEpoch 427/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8342 - loss: 0.7182 - val_accuracy: 0.8609 - val_loss: 2.1771\nEpoch 428/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8309 - loss: 0.7543 - val_accuracy: 0.8609 - val_loss: 2.1790\nEpoch 429/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8248 - loss: 0.8942 - val_accuracy: 0.8609 - val_loss: 2.1793\nEpoch 430/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8168 - loss: 0.8077 - val_accuracy: 0.8609 - val_loss: 2.1806\nEpoch 431/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8273 - loss: 0.7925 - val_accuracy: 0.8609 - val_loss: 2.1835\nEpoch 432/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8372 - loss: 0.7363 - val_accuracy: 0.8625 - val_loss: 2.1808\nEpoch 433/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8132 - loss: 0.8689 - val_accuracy: 0.8609 - val_loss: 2.1749\nEpoch 434/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8258 - loss: 0.7921 - val_accuracy: 0.8625 - val_loss: 2.1748\nEpoch 435/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8256 - loss: 0.7274 - val_accuracy: 0.8641 - val_loss: 2.1758\nEpoch 436/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8286 - loss: 0.7442 - val_accuracy: 0.8641 - val_loss: 2.1861\nEpoch 437/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8336 - loss: 0.8094 - val_accuracy: 0.8641 - val_loss: 2.1895\nEpoch 438/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8235 - loss: 0.8557 - val_accuracy: 0.8641 - val_loss: 2.1871\nEpoch 439/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8186 - loss: 0.9483 - val_accuracy: 0.8641 - val_loss: 2.1864\nEpoch 440/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8191 - loss: 0.7892 - val_accuracy: 0.8641 - val_loss: 2.1863\nEpoch 441/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8175 - loss: 0.9003 - val_accuracy: 0.8641 - val_loss: 2.1853\nEpoch 442/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8272 - loss: 0.8174 - val_accuracy: 0.8641 - val_loss: 2.1853\nEpoch 443/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8169 - loss: 0.8020 - val_accuracy: 0.8656 - val_loss: 2.1837\nEpoch 444/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8131 - loss: 0.7667 - val_accuracy: 0.8656 - val_loss: 2.1822\nEpoch 445/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8221 - loss: 0.7808 - val_accuracy: 0.8641 - val_loss: 2.1796\nEpoch 446/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8296 - loss: 0.6970 - val_accuracy: 0.8641 - val_loss: 2.1806\nEpoch 447/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8194 - loss: 0.7559 - val_accuracy: 0.8641 - val_loss: 2.1817\nEpoch 448/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8331 - loss: 0.6943 - val_accuracy: 0.8656 - val_loss: 2.1826\nEpoch 449/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8271 - loss: 0.7680 - val_accuracy: 0.8656 - val_loss: 2.1788\nEpoch 450/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8228 - loss: 0.7583 - val_accuracy: 0.8656 - val_loss: 2.1755\nEpoch 451/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8227 - loss: 0.8091 - val_accuracy: 0.8672 - val_loss: 2.1731\nEpoch 452/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8347 - loss: 0.7436 - val_accuracy: 0.8672 - val_loss: 2.1724\nEpoch 453/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8364 - loss: 0.7307 - val_accuracy: 0.8656 - val_loss: 2.1694\nEpoch 454/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8291 - loss: 0.8728 - val_accuracy: 0.8656 - val_loss: 2.1685\nEpoch 455/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8396 - loss: 0.9467 - val_accuracy: 0.8641 - val_loss: 2.1657\nEpoch 456/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8371 - loss: 0.7620 - val_accuracy: 0.8656 - val_loss: 2.1661\nEpoch 457/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8342 - loss: 0.8619 - val_accuracy: 0.8656 - val_loss: 2.1639\nEpoch 458/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8322 - loss: 0.7181 - val_accuracy: 0.8672 - val_loss: 2.1634\nEpoch 459/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8234 - loss: 0.7914 - val_accuracy: 0.8672 - val_loss: 2.1642\nEpoch 460/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8379 - loss: 0.8679 - val_accuracy: 0.8656 - val_loss: 2.1771\nEpoch 461/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8352 - loss: 0.7035 - val_accuracy: 0.8656 - val_loss: 2.1811\nEpoch 462/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8244 - loss: 0.7367 - val_accuracy: 0.8656 - val_loss: 2.1849\nEpoch 463/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8292 - loss: 0.7991 - val_accuracy: 0.8656 - val_loss: 2.1891\nEpoch 464/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8325 - loss: 0.7287 - val_accuracy: 0.8641 - val_loss: 2.1885\nEpoch 465/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8274 - loss: 0.8833 - val_accuracy: 0.8656 - val_loss: 2.1902\nEpoch 466/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8335 - loss: 0.6852 - val_accuracy: 0.8687 - val_loss: 2.1925\nEpoch 467/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8305 - loss: 0.7616 - val_accuracy: 0.8672 - val_loss: 2.1957\nEpoch 468/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8267 - loss: 0.7998 - val_accuracy: 0.8656 - val_loss: 2.1963\nEpoch 469/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8470 - loss: 0.6237 - val_accuracy: 0.8672 - val_loss: 2.1999\nEpoch 470/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8439 - loss: 0.6939 - val_accuracy: 0.8687 - val_loss: 2.2052\nEpoch 471/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8382 - loss: 0.7419 - val_accuracy: 0.8687 - val_loss: 2.2069\nEpoch 472/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8505 - loss: 0.6306 - val_accuracy: 0.8687 - val_loss: 2.2076\nEpoch 473/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8344 - loss: 0.6796 - val_accuracy: 0.8687 - val_loss: 2.2046\nEpoch 474/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8431 - loss: 0.7194 - val_accuracy: 0.8687 - val_loss: 2.2034\nEpoch 475/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8392 - loss: 0.6636 - val_accuracy: 0.8687 - val_loss: 2.2005\nEpoch 476/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8385 - loss: 0.7055 - val_accuracy: 0.8687 - val_loss: 2.2043\nEpoch 477/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8346 - loss: 0.7685 - val_accuracy: 0.8687 - val_loss: 2.2055\nEpoch 478/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8407 - loss: 1.0403 - val_accuracy: 0.8687 - val_loss: 2.2066\nEpoch 479/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8385 - loss: 0.6773 - val_accuracy: 0.8687 - val_loss: 2.2055\nEpoch 480/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8381 - loss: 0.7511 - val_accuracy: 0.8687 - val_loss: 2.2038\nEpoch 481/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8390 - loss: 0.6770 - val_accuracy: 0.8687 - val_loss: 2.2071\nEpoch 482/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8318 - loss: 0.7938 - val_accuracy: 0.8687 - val_loss: 2.2077\nEpoch 483/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8317 - loss: 0.6589 - val_accuracy: 0.8672 - val_loss: 2.2075\nEpoch 484/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8365 - loss: 0.6740 - val_accuracy: 0.8687 - val_loss: 2.2082\nEpoch 485/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8345 - loss: 0.7737 - val_accuracy: 0.8687 - val_loss: 2.2132\nEpoch 486/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8332 - loss: 0.7549 - val_accuracy: 0.8687 - val_loss: 2.2135\nEpoch 487/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8386 - loss: 0.7722 - val_accuracy: 0.8703 - val_loss: 2.2136\nEpoch 488/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8468 - loss: 0.6540 - val_accuracy: 0.8719 - val_loss: 2.2162\nEpoch 489/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8470 - loss: 0.7171 - val_accuracy: 0.8703 - val_loss: 2.2097\nEpoch 490/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8378 - loss: 0.6791 - val_accuracy: 0.8719 - val_loss: 2.2078\nEpoch 491/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8451 - loss: 0.6932 - val_accuracy: 0.8719 - val_loss: 2.2039\nEpoch 492/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8382 - loss: 0.7130 - val_accuracy: 0.8719 - val_loss: 2.2067\nEpoch 493/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8414 - loss: 0.7384 - val_accuracy: 0.8734 - val_loss: 2.2051\nEpoch 494/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8447 - loss: 0.6841 - val_accuracy: 0.8734 - val_loss: 2.2044\nEpoch 495/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8391 - loss: 0.6679 - val_accuracy: 0.8734 - val_loss: 2.2046\nEpoch 496/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8338 - loss: 0.7660 - val_accuracy: 0.8703 - val_loss: 2.2042\nEpoch 497/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8418 - loss: 0.6563 - val_accuracy: 0.8703 - val_loss: 2.2089\nEpoch 498/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8395 - loss: 0.9708 - val_accuracy: 0.8703 - val_loss: 2.2109\nEpoch 499/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8366 - loss: 0.7444 - val_accuracy: 0.8719 - val_loss: 2.2119\nEpoch 500/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8455 - loss: 0.6333 - val_accuracy: 0.8734 - val_loss: 2.2135\nEpoch 501/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8451 - loss: 0.6591 - val_accuracy: 0.8734 - val_loss: 2.2176\nEpoch 502/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8412 - loss: 0.6304 - val_accuracy: 0.8734 - val_loss: 2.2171\nEpoch 503/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8323 - loss: 1.0082 - val_accuracy: 0.8687 - val_loss: 2.2150\nEpoch 504/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8400 - loss: 0.6763 - val_accuracy: 0.8687 - val_loss: 2.2191\nEpoch 505/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8468 - loss: 1.7952 - val_accuracy: 0.8672 - val_loss: 2.2127\nEpoch 506/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8472 - loss: 0.6925 - val_accuracy: 0.8687 - val_loss: 2.2143\nEpoch 507/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 205ms/step - accuracy: 0.8397 - loss: 0.6795 - val_accuracy: 0.8687 - val_loss: 2.2198\nEpoch 508/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8389 - loss: 5.1175 - val_accuracy: 0.8687 - val_loss: 2.2341\nEpoch 509/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8423 - loss: 0.7232 - val_accuracy: 0.8672 - val_loss: 2.2578\nEpoch 510/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8402 - loss: 0.6776 - val_accuracy: 0.8672 - val_loss: 2.2639\nEpoch 511/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8326 - loss: 0.7637 - val_accuracy: 0.8672 - val_loss: 2.2629\nEpoch 512/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8404 - loss: 0.6379 - val_accuracy: 0.8672 - val_loss: 2.2640\nEpoch 513/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8388 - loss: 0.8354 - val_accuracy: 0.8672 - val_loss: 2.2638\nEpoch 514/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8394 - loss: 0.6212 - val_accuracy: 0.8672 - val_loss: 2.2666\nEpoch 515/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8508 - loss: 0.6137 - val_accuracy: 0.8687 - val_loss: 2.2691\nEpoch 516/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8351 - loss: 0.6883 - val_accuracy: 0.8687 - val_loss: 2.2720\nEpoch 517/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8434 - loss: 0.6390 - val_accuracy: 0.8703 - val_loss: 2.2744\nEpoch 518/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8324 - loss: 0.7160 - val_accuracy: 0.8687 - val_loss: 2.2726\nEpoch 519/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8433 - loss: 0.6422 - val_accuracy: 0.8687 - val_loss: 2.2703\nEpoch 520/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8424 - loss: 0.7340 - val_accuracy: 0.8687 - val_loss: 2.2588\nEpoch 521/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8413 - loss: 0.6473 - val_accuracy: 0.8687 - val_loss: 2.2594\nEpoch 522/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8412 - loss: 0.7337 - val_accuracy: 0.8703 - val_loss: 2.2609\nEpoch 523/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8518 - loss: 0.6679 - val_accuracy: 0.8703 - val_loss: 2.2623\nEpoch 524/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8509 - loss: 0.8157 - val_accuracy: 0.8703 - val_loss: 2.2674\nEpoch 525/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8457 - loss: 0.8130 - val_accuracy: 0.8703 - val_loss: 2.2674\nEpoch 526/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8557 - loss: 0.5823 - val_accuracy: 0.8734 - val_loss: 2.2699\nEpoch 527/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8368 - loss: 0.6983 - val_accuracy: 0.8734 - val_loss: 2.2778\nEpoch 528/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8436 - loss: 0.6769 - val_accuracy: 0.8734 - val_loss: 2.2791\nEpoch 529/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8534 - loss: 0.5974 - val_accuracy: 0.8734 - val_loss: 2.2820\nEpoch 530/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8334 - loss: 0.7379 - val_accuracy: 0.8734 - val_loss: 2.2833\nEpoch 531/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8561 - loss: 0.6259 - val_accuracy: 0.8750 - val_loss: 2.2876\nEpoch 532/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8516 - loss: 0.6632 - val_accuracy: 0.8750 - val_loss: 2.2875\nEpoch 533/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8455 - loss: 0.6698 - val_accuracy: 0.8750 - val_loss: 2.2846\nEpoch 534/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8482 - loss: 0.6323 - val_accuracy: 0.8750 - val_loss: 2.2799\nEpoch 535/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8524 - loss: 0.6370 - val_accuracy: 0.8750 - val_loss: 2.2779\nEpoch 536/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8493 - loss: 0.7792 - val_accuracy: 0.8734 - val_loss: 2.2693\nEpoch 537/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8408 - loss: 0.6492 - val_accuracy: 0.8734 - val_loss: 2.2655\nEpoch 538/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8482 - loss: 0.5806 - val_accuracy: 0.8734 - val_loss: 2.2640\nEpoch 539/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8541 - loss: 0.5938 - val_accuracy: 0.8734 - val_loss: 2.2589\nEpoch 540/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8427 - loss: 0.6335 - val_accuracy: 0.8750 - val_loss: 2.2661\nEpoch 541/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8502 - loss: 0.5975 - val_accuracy: 0.8766 - val_loss: 2.2745\nEpoch 542/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8412 - loss: 0.7291 - val_accuracy: 0.8766 - val_loss: 2.2711\nEpoch 543/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8449 - loss: 0.6393 - val_accuracy: 0.8766 - val_loss: 2.2722\nEpoch 544/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8526 - loss: 0.6516 - val_accuracy: 0.8766 - val_loss: 2.2746\nEpoch 545/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8458 - loss: 0.6832 - val_accuracy: 0.8766 - val_loss: 2.2709\nEpoch 546/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8546 - loss: 0.5790 - val_accuracy: 0.8766 - val_loss: 2.2719\nEpoch 547/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8448 - loss: 0.6846 - val_accuracy: 0.8766 - val_loss: 2.2715\nEpoch 548/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8578 - loss: 0.6512 - val_accuracy: 0.8766 - val_loss: 2.2733\nEpoch 549/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8389 - loss: 0.6414 - val_accuracy: 0.8766 - val_loss: 2.2759\nEpoch 550/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8499 - loss: 0.6458 - val_accuracy: 0.8766 - val_loss: 2.2730\nEpoch 551/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8521 - loss: 0.5807 - val_accuracy: 0.8766 - val_loss: 2.2706\nEpoch 552/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8642 - loss: 0.6069 - val_accuracy: 0.8766 - val_loss: 2.2687\nEpoch 553/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8623 - loss: 0.6010 - val_accuracy: 0.8766 - val_loss: 2.2697\nEpoch 554/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8451 - loss: 0.9123 - val_accuracy: 0.8766 - val_loss: 2.2605\nEpoch 555/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8612 - loss: 0.5707 - val_accuracy: 0.8766 - val_loss: 2.2606\nEpoch 556/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8545 - loss: 0.5792 - val_accuracy: 0.8766 - val_loss: 2.2644\nEpoch 557/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8537 - loss: 0.6283 - val_accuracy: 0.8766 - val_loss: 2.2716\nEpoch 558/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8549 - loss: 0.6280 - val_accuracy: 0.8766 - val_loss: 2.2747\nEpoch 559/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8429 - loss: 0.7594 - val_accuracy: 0.8766 - val_loss: 2.2746\nEpoch 560/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8611 - loss: 0.6164 - val_accuracy: 0.8766 - val_loss: 2.2788\nEpoch 561/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8562 - loss: 0.6267 - val_accuracy: 0.8766 - val_loss: 2.2870\nEpoch 562/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8535 - loss: 0.6451 - val_accuracy: 0.8781 - val_loss: 2.2894\nEpoch 563/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8486 - loss: 0.6261 - val_accuracy: 0.8781 - val_loss: 2.2917\nEpoch 564/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8516 - loss: 0.6412 - val_accuracy: 0.8781 - val_loss: 2.2881\nEpoch 565/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8519 - loss: 0.6905 - val_accuracy: 0.8766 - val_loss: 2.2851\nEpoch 566/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8578 - loss: 0.5237 - val_accuracy: 0.8766 - val_loss: 2.2883\nEpoch 567/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.8408 - loss: 0.6756 - val_accuracy: 0.8766 - val_loss: 2.2888\nEpoch 568/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.8579 - loss: 0.5991 - val_accuracy: 0.8766 - val_loss: 2.2915\nEpoch 569/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 203ms/step - accuracy: 0.8612 - loss: 0.5628 - val_accuracy: 0.8766 - val_loss: 2.2911\nEpoch 570/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - accuracy: 0.8496 - loss: 0.6892 - val_accuracy: 0.8766 - val_loss: 2.2878\nEpoch 571/2000\n\u001b[1m5/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 201ms/step - accuracy: 0.8588 - loss: 0.6217 - val_accuracy: 0.8781 - val_loss: 2.2922\nEpoch 572/2000\n\u001b[1m1/5\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 219ms/step - accuracy: 0.8535 - loss: 0.6314","output_type":"stream"}]},{"cell_type":"code","source":"n_epochs = len(history.history['loss'])\n\nprint(f\"The model was trained for {n_epochs} epochs.\")\n\n# Evaluate the model on the train set  \ntest_loss, test_acc = model.evaluate(X_train, Y_train,verbose=2)\n\nprint(f'\\nTest accuracy on training data: {test_acc} \\n \\n')\n\n# Evaluate the model on the test set\ntest_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint(f'\\nTest accuracy on test data: {test_acc}\\n \\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot([i for i in range(n_epochs)], history.history['accuracy'],label='Train')\nplt.plot([i for i in range(n_epochs)], history.history['val_accuracy'],label='Val')\nplt.legend()\n# plt.ylim(0,1)\nplt.xlabel('epochs')\nplt.ylabel('Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot([i for i in range(n_epochs)], history.history['loss'],label='Train')\nplt.plot([i for i in range(n_epochs)], history.history['val_loss'],label='val')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ntest_prediction = model.predict(X_test)\ntest_predicted_label = np.argmax(test_prediction, axis=1)\ntest_actual_label = np.argmax(Y_test, axis = 1)\nconf_m = confusion_matrix(test_actual_label, test_predicted_label)\n\nconf_m_show = ConfusionMatrixDisplay(confusion_matrix = conf_m,display_labels=['GRB','TGF','SGR','SFLARE'])\nconf_m_show.plot(include_values=True, cmap='Blues',ax = None, xticks_rotation= 'horizontal')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# saving the model   \nmodel.save('DCL.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/working/trainHistoryDict', 'wb') as file_pi:\n    pickle.dump(history.history, file_pi)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink('DCL.keras')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndef model_out(model, X, Y):\n    preds = []\n    bkgd_int = [(0, 0), (0, 0)]\n    poly_column = np.array([0, 0])\n    sig = 0.0\n    bin_size = '1'\n\n    x_col = []\n    y_col = []\n    event_types = []\n    event_names = []\n\n    if isinstance(X, dict):\n        for n in range(len(Y)):\n            x = {}\n            for key in X.keys():\n                arr = X[key][n]\n                arr = arr.reshape(1, arr.shape[0], arr.shape[-1])\n                x[key] = arr\n            y = Y[n].tolist()\n\n            y_pred = model.predict(x, verbose=0).tolist()[0]\n\n            x_col.append(x)\n            y_col.append(y)\n            preds.append(y_pred)\n    else:\n        print('need dict')\n        return None\n\n    # Create the DataFrame\n    df = pd.DataFrame({\n        'x': x_col,\n        'y': y_col,\n        'prediction': preds,\n        \n    })\n\n    return df,preds\n\ndf,preds = model_out(model, X_test, Y_test)\nprint('value counts',df['y'].value_counts())\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calculate_cnn_precision_multiclass(y_true, y_pred):\n    \"\"\"\n    Calculate the precision of a CNN model for a 4-class classifier using one-hot encoding.\n    \n    Args:\n    y_true (numpy.ndarray): Ground truth labels (one-hot encoded)\n    y_pred (numpy.ndarray): Predicted probabilities for each class\n    \n    Returns:\n    numpy.ndarray: Precision score for each class\n    float: Average precision across all classes\n    \"\"\"\n    \n    # Convert predicted probabilities to class predictions\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    \n    # Number of classes\n    num_classes = y_true.shape[1]\n    \n    # Initialize arrays to store true positives and predicted positives\n    true_positives = np.zeros(num_classes)\n    predicted_positives = np.zeros(num_classes)\n    \n    # Calculate true positives and predicted positives for each class\n    for i in range(num_classes):\n        true_positives[i] = np.sum((np.argmax(y_true, axis=1) == i) & (y_pred_classes == i))\n        predicted_positives[i] = np.sum(y_pred_classes == i)\n    \n    # Calculate precision for each class\n    precision = true_positives / (predicted_positives + 1e-7)\n    \n    # Calculate average precision\n    average_precision = np.mean(precision)\n    \n    return precision, average_precision\n\ndef calculate_cnn_recall_multiclass(y_true, y_pred):\n    \"\"\"\n    Calculate the recall of a CNN model for a 4-class classifier using one-hot encoding.\n    \n    Args:\n    y_true (numpy.ndarray): Ground truth labels (one-hot encoded)\n    y_pred (numpy.ndarray): Predicted probabilities for each class\n    \n    Returns:\n    numpy.ndarray: Recall score for each class\n    float: Average recall across all classes\n    \"\"\"\n    \n    # Convert predicted probabilities to class predictions\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    \n    # Number of classes\n    num_classes = y_true.shape[1]\n    \n    # Initialize arrays to store true positives and actual positives\n    true_positives = np.zeros(num_classes)\n    actual_positives = np.zeros(num_classes)\n    \n    # Calculate true positives and actual positives for each class\n    for i in range(num_classes):\n        true_positives[i] = np.sum((np.argmax(y_true, axis=1) == i) & (y_pred_classes == i))\n        actual_positives[i] = np.sum(np.argmax(y_true, axis=1) == i)\n    \n    # Calculate recall for each class\n    recall = true_positives / (actual_positives + 1e-7)\n    \n    # Calculate average recall\n    average_recall = np.mean(recall)\n    \n    return recall, average_recall\n\ndef calculate_cnn_f1_score_multiclass(y_true, y_pred):\n    \"\"\"\n    Calculate the F1 score of a CNN model for a 4-class classifier using one-hot encoding.\n    \n    Args:\n    y_true (numpy.ndarray): Ground truth labels (one-hot encoded)\n    y_pred (numpy.ndarray): Predicted probabilities for each class\n    \n    Returns:\n    numpy.ndarray: F1 score for each class\n    float: Average F1 score across all classes\n    \"\"\"\n    \n    # Calculate precision and recall\n    precision, _ = calculate_cnn_precision_multiclass(y_true, y_pred)\n    recall, _ = calculate_cnn_recall_multiclass(y_true, y_pred)\n    \n    # Calculate F1 score for each class\n    f1_score = 2 * (precision * recall) / (precision + recall + 1e-7)\n    \n    # Calculate average F1 score\n    average_f1_score = np.mean(f1_score)\n    \n    return f1_score, average_f1_score\n\ndef calculate_cnn_accuracy_multiclass(y_true, y_pred):\n    \"\"\"\n    Calculate the accuracy of a CNN model for a 4-class classifier using one-hot encoding.\n    \n    Args:\n    y_true (numpy.ndarray): Ground truth labels (one-hot encoded)\n    y_pred (numpy.ndarray): Predicted probabilities for each class\n    \n    Returns:\n    numpy.ndarray: Accuracy score for each class\n    float: Overall accuracy across all classes\n    \"\"\"\n    \n    # Convert predicted probabilities to class predictions\n    y_pred_classes = np.argmax(y_pred, axis=1)\n    y_true_classes = np.argmax(y_true, axis=1)\n    \n    # Number of classes\n    num_classes = y_true.shape[1]\n    \n    # Initialize arrays to store correct predictions and total predictions\n    correct_predictions = np.zeros(num_classes)\n    total_predictions = np.zeros(num_classes)\n    \n    # Calculate correct predictions and total predictions for each class\n    for i in range(num_classes):\n        correct_predictions[i] = np.sum((y_true_classes == i) & (y_pred_classes == i))\n        total_predictions[i] = np.sum(y_true_classes == i)\n    \n    # Calculate accuracy for each class\n    class_accuracy = correct_predictions / (total_predictions + 1e-7)\n    \n    # Calculate overall accuracy\n    overall_accuracy = np.sum(correct_predictions) / len(y_true_classes)\n    \n    return class_accuracy, overall_accuracy","metadata":{"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_acc,overall_acc = calculate_cnn_accuracy_multiclass(Y_test,preds)\npre, avg_pre = calculate_cnn_precision_multiclass(Y_test,preds)\nrecall, average_reacall = calculate_cnn_recall_multiclass(Y_test,preds)\nf1_score, average_f1_score = calculate_cnn_f1_score_multiclass(Y_test,preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tabulate import tabulate\nimport numpy as np\n\nmetrics = [\n    [\"Accuracy\"] + list(class_acc) + [overall_acc],\n    [\"Precision\"] + list(pre) + [avg_pre],\n    [\"Recall\"] + list(recall) + [average_reacall],\n    [\"F1 Score\"] + list(f1_score) + [average_f1_score]\n]\n\nheaders = [\"Metric\", \"GRB\", \"TGF\", \"SGR\", \"SFLARE\", \"Overall/Average\"]\ntable = tabulate(metrics, headers=headers, tablefmt=\"grid\")\nprint(table)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}