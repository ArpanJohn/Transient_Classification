{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCL CNN\n",
    "\n",
    "# importing all needed functions\n",
    "import os\n",
    "from pathlib import Path\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "from Tools import tools\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit\n",
    "import os\n",
    "import shutil\n",
    "import line_profiler\n",
    "from scipy import stats\n",
    "from line_profiler import profile\n",
    "%load_ext line_profiler\n",
    "import pwkit.bblocks\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Polyfit may be poorly conditioned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TGF_bn190821888', 'SFLARE_bn220520007', 'GRB_bn100224112', 'TGF_bn151209879', 'SGR_bn220114673', 'SFLARE_bn110309971', 'TGF_bn100331421', 'TGF_bn150209374', 'SFLARE_bn130421669', 'SGR_bn160623809']\n",
      "['SFLARE_bn140204646', 'SFLARE_bn240208118', 'SFLARE_bn240204601', 'TGF_bn221114697', 'SGR_bn211226538', 'TGF_bn100901124', 'GRB_bn150403913', 'TGF_bn170911550', 'TGF_bn101012231', 'SGR_bn160623838']\n"
     ]
    }
   ],
   "source": [
    "# reading the ttsplit file\n",
    "df = pd.read_csv('ttsplit')\n",
    "\n",
    "# getting the test and train events\n",
    "test_events = df[df['category'] == 'test']['filename'].tolist()\n",
    "print(test_events[:10])  \n",
    "\n",
    "train_events = df[df['category'] == 'train']['filename'].tolist()\n",
    "print(train_events[:10])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_files = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_folders(directory, folder_list):\n",
    "    for folder in folder_list:\n",
    "        folder_path = os.path.join(directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            try:\n",
    "                shutil.rmtree(folder_path)\n",
    "                print(f\"Deleted folder: {folder}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error deleting folder {folder}: {str(e)}\")\n",
    "        else:\n",
    "            print(f\"{folder} is not a folder or does not exist.\")\n",
    "\n",
    "@jit(nopython=True)\n",
    "def filter_counts(times, channels, min_ch, max_ch, trigtime):\n",
    "    \"\"\"\n",
    "    Filter count data based on channel range and subtract trigtime.\n",
    "    \n",
    "    :param times: Array of event times\n",
    "    :param channels: Array of channel numbers\n",
    "    :param min_ch: Minimum channel number\n",
    "    :param max_ch: Maximum channel number\n",
    "    :param trigtime: Trigger time to be subtracted\n",
    "    :return: Filtered and adjusted count data\n",
    "    \"\"\"\n",
    "    counts = []\n",
    "    for t, ch in zip(times, channels):\n",
    "        if min_ch <= ch <= max_ch:\n",
    "            counts.append(t - trigtime)\n",
    "    return np.array(counts)\n",
    "\n",
    "@profile\n",
    "def process_folder(folder, bin_list,data_no,r,chrs,data_set_path,source_data_set_path,error_folders,inup=False,zero_files = zero_files, plot = False):\n",
    "    \"\"\"\n",
    "    Process a single folder of data.\n",
    "    \n",
    "    :param folder: Name of the folder to process\n",
    "    :param bin_list: List of bin sizes to use\n",
    "    \"\"\"\n",
    "    # try:\n",
    "    # Extract event information from folder name\n",
    "    event_type, event = folder.split(\"_\")\n",
    "\n",
    "    mod_folder = folder[:-1] if folder [-1] == 'd' else folder\n",
    "\n",
    "\n",
    "    # check if the file is processed\n",
    "    data_file_path = Path(data_set_path) / f\"{event_type}_{event}\"\n",
    "    if data_file_path.exists():\n",
    "        # print(f\"File already processed: {data_file_path}\")\n",
    "        return\n",
    "\n",
    "    # Construct file pattern and find matching files\n",
    "    file_pattern = str(Path(source_data_set_path) / mod_folder / 'current' / '*_tte_*')\n",
    "    NaI_detector = glob.glob(file_pattern)\n",
    "\n",
    "\n",
    "    # Read data from FITS file\n",
    "    with fits.open(NaI_detector[0], memmap=True) as hdul:\n",
    "        all_count_data = hdul[2].data\n",
    "        trigtime = float(hdul[2].header['TRIGTIME'])\n",
    "\n",
    "    # Extract times and channels from all_count_data\n",
    "    times = all_count_data['TIME'].astype(float)\n",
    "    channels = all_count_data['PHA'].astype(int)\n",
    "\n",
    "    counts_list = []\n",
    "\n",
    "    for ranges in chrs:\n",
    "        # Filter counts for different channel ranges\n",
    "        counts_list.append(filter_counts(times, channels, ranges[0], ranges[1], trigtime))        \n",
    "\n",
    "    data_array = []\n",
    "    \n",
    "    # Process both count ranges\n",
    "    for i in bin_list:\n",
    "        sep_sig = []\n",
    "        ch_no = 1\n",
    "        for counts in counts_list:\n",
    "            # Calculate range and bin size\n",
    "            range_min = -data_no * i * r\n",
    "            range_max = data_no * i * (1-r)\n",
    "            bin_size = i\n",
    "\n",
    "            # print(range_max)\n",
    "            # print(range_min)\n",
    "\n",
    "            # Create histogram\n",
    "            bin_edges = np.arange(range_min, range_max, bin_size)\n",
    "            hist, edges = np.histogram(counts, bins=bin_edges)\n",
    "            hist = adjust_zeros(hist)\n",
    "            bin_widths = edges[1:]-edges[:-1]\n",
    "            hrates = hist / bin_size\n",
    "\n",
    "            up_rates = hrates # unprocessed rates\n",
    "            if inup:\n",
    "                sep_sig.extend(up_rates)\n",
    "\n",
    "            # Perform Bayesian Block Analysis\n",
    "            bayes = pwkit.bblocks.bin_bblock(widths = bin_widths, counts = hist, p0 = 0.01)\n",
    "\n",
    "            # Getting the results of the analysis\n",
    "            block_starts = (bayes.get('blockstarts') * i) + range_min\n",
    "            block_stops = block_starts + bayes.get('widths')\n",
    "            bins = np.array(list(block_starts) + list([block_stops[-1]]))\n",
    "            rates = bayes.get('rates')\n",
    "\n",
    "            # Find indices where rates are non-zero\n",
    "            non_zero_indices = np.nonzero(rates)[0]\n",
    "            \n",
    "            # Select rates that are non-zero\n",
    "            rates = rates[non_zero_indices]\n",
    "            \n",
    "            try:\n",
    "                # Select corresponding bins\n",
    "                bins = np.concatenate([bins[non_zero_indices], [bins[non_zero_indices[-1] + 1]]])\n",
    "            except:\n",
    "                bins = [0,0,0,0]\n",
    "                \n",
    "            sigw = bins[-2] - bins[1]\n",
    "\n",
    "            bk2 = bins[1] - (5 * bin_size)\n",
    "            bk3 = bins[-2] + (5 * bin_size)\n",
    "            bk1 = bk2 - sigw - (50 * bin_size) if (bk2 - sigw - (50 * bin_size)) > bins[0] else bins[0]\n",
    "            bk4 = bk3 + sigw + (50 * bin_size) if (bk3 + sigw + (50 * bin_size)) < bins[-1] else bins[-1]\n",
    "\n",
    "            # bk2 = bins[1]  # \n",
    "            # bk3 = bins[-2] # \n",
    "            # bk1 = bins[0]  # \n",
    "            # bk4 = bins[-1] # \n",
    "            \n",
    "            bkgd = [(bk1,bk2),(bk3,bk4)]\n",
    "\n",
    "            # Check if background is proper\n",
    "            bkgd_check = bk1<bk2<bk3<bk4\n",
    "\n",
    "            # Checking for signal\n",
    "            # Checks False alarm probability p0, number of bins, and if background is detected around trigger\n",
    "            if bayes.get('finalp0') >= 0.05 or len(bins)<4 or not(bk2 - 5 <= 0 <= bk3 + 5) or not bkgd_check:\n",
    "                if plot:\n",
    "                    print('no signal')\n",
    "                    plt.stairs(hrates, edges)\n",
    "                    plt.show()\n",
    "                    plt.stairs(rates, bins)\n",
    "                    plt.show()\n",
    "                # print('no signal')\n",
    "                # print(i,ch_no)\n",
    "                signal = np.zeros(data_no-1).astype(np.int32) # send no signal\n",
    "                                \n",
    "            else:\n",
    "                y = hrates\n",
    "                x = (edges[1:] + edges[:-1]) / 2\n",
    "                bkgd_pol,_ = best_fit_polynomial(x, y, bkgd)\n",
    "\n",
    "                # Create mask for y between bk2 and bk3\n",
    "                mask = (x >= bk2) & (x <= bk3)\n",
    "\n",
    "                # Apply the mask and take the maximum of 0 and y - bkgd_pol(x)\n",
    "                signal = np.zeros(data_no-1).astype(np.int32)\n",
    "                signal[mask] = np.maximum(0, (y[mask] - bkgd_pol(x[mask]))).astype(np.int32)\n",
    "                \n",
    "                if plot:\n",
    "                    # Plot the results\n",
    "                    plt.stairs(hrates,edges)\n",
    "                    plt.xlabel('Time (s)')\n",
    "                    plt.ylabel('rates')\n",
    "                    plt.title(f'light curve ; ch = {ch_no} ;bin size = {bin_size}s')\n",
    "                    # Plot each shaded area\n",
    "                    for i, (x1, x2) in enumerate(bkgd):\n",
    "                        plt.axvspan(x1, x2, alpha=0.3, color='red',label = 'background selection') \n",
    "                    # make sure most things are visible\n",
    "                    if min(edges) < counts.min():\n",
    "                        plt.gca().set_xlim(left=counts.min())\n",
    "                    if max(edges) > counts.max():\n",
    "                        plt.gca().set_xlim(right=counts.max())\n",
    "                    plt.axvline(x=0, color='red', linestyle='--', linewidth=0.5)\n",
    "                    # Evaluate the background polynomial for plotting\n",
    "                    bkgd_y = bkgd_pol(x)\n",
    "                    \n",
    "                    # Check if the background blows up\n",
    "                    if np.any(np.abs(bkgd_y) > 10 * np.max(np.abs(y))) or np.any(np.abs(bkgd_y) < 0.1 * np.min(np.abs(y[y != 0]))):                \n",
    "                        # Plot the background within the limits\n",
    "                        mask = (bkgd_y >= plt.ylim()[0]) & (bkgd_y <= plt.ylim()[1])\n",
    "                        plt.plot(x[mask], bkgd_y[mask], '--', label='background')\n",
    "                    else:\n",
    "                        # If it doesn't blow up, plot normally\n",
    "                        plt.plot(x, bkgd_y, '--', label='background')\n",
    "                    plt.show()\n",
    "\n",
    "                    # signal plot\n",
    "                    plt.title(f'light curve ; ch = {ch_no} ;bin size = {bin_size}s')\n",
    "                    plt.axvline(x=0, color='red', linestyle='--', linewidth=0.5)\n",
    "                    plt.xlabel('Time (s)')\n",
    "                    plt.ylabel('rates')\n",
    "                    plt.plot(x,signal)\n",
    "                    plt.show()\n",
    "\n",
    "            # updatae channel number and save signal\n",
    "            ch_no = ch_no + 1               \n",
    "            sep_sig.extend(signal)\n",
    "            \n",
    "\n",
    "        # Accumulate histogram data\n",
    "        data_array.extend(sep_sig)\n",
    "\n",
    "    # Convert accumulated data to numpy array\n",
    "    data_array = np.array(data_array)\n",
    "\n",
    "    extra = 2 if inup else 1\n",
    "    if data_array.shape[0] != (len(bin_list)) * (len(chrs)*extra) * (data_no-1):\n",
    "        print('something may have gone wrong in ', folder)\n",
    "        print(data_array.shape[0], (len(bin_list)) * (len(chrs)*(extra)) * (data_no-1))\n",
    "        print(type(data_array.shape[0]), type((len(bin_list)) * (len(chrs)*(extra)) * (data_no-1)))\n",
    "        print(data_array.shape[0] != (len(bin_list)) * (len(chrs)+extra) * (data_no-1))\n",
    "    \n",
    "    if plot:\n",
    "        print('shape', data_array.shape)\n",
    "\n",
    "    if np.all(data_array == 0):\n",
    "        zero_files = zero_files + 1\n",
    "\n",
    "    # print('shape', data_array.shape)\n",
    "\n",
    "    # Save data to file\n",
    "    with open(data_file_path, 'w') as f:\n",
    "        np.savetxt(f, data_array, fmt='%d', delimiter='')\n",
    "        # print(f\"Data saved to {data_file_path}\")\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     # Handle and log any errors\n",
    "    #     print(f'error {e} in {folder}')\n",
    "    #     # traceback.print_exc()\n",
    "    #     error_folders.append(folder)     \n",
    "\n",
    "    return zero_files   \n",
    "\n",
    "def adjust_zeros(arr):\n",
    "    # Create a copy of the input array\n",
    "    result = np.copy(arr)\n",
    "    \n",
    "    # Find first non-zero element\n",
    "    first_non_zero = np.argmax(arr != 0)\n",
    "    \n",
    "    # Find last non-zero element\n",
    "    last_non_zero = len(arr) - np.argmax(arr[::-1] != 0) - 1\n",
    "    \n",
    "    # Replace value after leading zeros with zero if there are leading zeros\n",
    "    if first_non_zero > 0:\n",
    "        result[first_non_zero] = 0\n",
    "    \n",
    "    # Replace value before trailing zeros with zero if there are trailing zeros\n",
    "    if last_non_zero < len(arr) - 1:\n",
    "        result[last_non_zero] = 0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def best_fit_polynomial(x, y, background_intervals):\n",
    "    # Filter data to include only background intervals\n",
    "    mask = np.zeros(len(x), dtype=bool)\n",
    "    for start, end in background_intervals:\n",
    "        mask |= (x >= start) & (x <= end)\n",
    "    \n",
    "    x_bg = x[mask]\n",
    "    y_bg = y[mask]\n",
    "    \n",
    "    best_order = 0\n",
    "    best_polynomial = None\n",
    "    best_r_squared = -np.inf\n",
    "    \n",
    "    for order in range(1, 5):  # Test polynomials of order 1 to 4\n",
    "        coeffs = np.polyfit(x_bg, y_bg, order)\n",
    "        p = np.poly1d(coeffs)\n",
    "        \n",
    "        # Calculate R-squared\n",
    "        y_pred = p(x_bg)\n",
    "        r_squared = 1 - np.sum((y_bg - y_pred)**2) / np.sum((y_bg - np.mean(y_bg))**2)\n",
    "        \n",
    "        # Perform F-test to compare with lower order model\n",
    "        if order > 1:\n",
    "            f_statistic = ((r_squared - prev_r_squared) / (order - prev_order)) / \\\n",
    "                          ((1 - r_squared) / (len(x_bg) - order - 1))\n",
    "            p_value = 1 - stats.f.cdf(f_statistic, 1, len(x_bg) - order - 1)\n",
    "            \n",
    "            if p_value < 0.05 and r_squared > best_r_squared:\n",
    "                best_order = order\n",
    "                best_polynomial = p\n",
    "                best_r_squared = r_squared\n",
    "        else:\n",
    "            best_order = order\n",
    "            best_polynomial = p\n",
    "            best_r_squared = r_squared\n",
    "        \n",
    "        prev_order = order\n",
    "        prev_r_squared = r_squared\n",
    "    \n",
    "    return best_polynomial, best_order\n",
    "\n",
    "def full_plot(x,y,bkgd,bkgd_pol,peak = 0,peak_time = 0):\n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(x,y,label = 'data')\n",
    "\n",
    "        # Plot each shaded area\n",
    "        for i, (x1, x2) in enumerate(bkgd):\n",
    "            plt.axvspan(x1, x2, alpha=0.3, color='red',label = 'background selection') \n",
    "        \n",
    "        check_poly = np.poly1d([0,1])\n",
    "        if not np.all(bkgd_pol.coef == check_poly.coef):\n",
    "            # Evaluate the background polynomial\n",
    "            bkgd_y = bkgd_pol(x)\n",
    "            \n",
    "            # Check if the background blows up\n",
    "            if np.any(np.abs(bkgd_y) > 10 * np.max(np.abs(y))) or np.any(np.abs(bkgd_y) < 0.1 * np.min(np.abs(y[y != 0]))):                \n",
    "                # Plot the background within the limits\n",
    "                mask = (bkgd_y >= plt.ylim()[0]) & (bkgd_y <= plt.ylim()[1])\n",
    "                plt.plot(x[mask], bkgd_y[mask], '--', label='background')\n",
    "            else:\n",
    "                # If it doesn't blow up, plot normally\n",
    "                plt.plot(x, bkgd_y, '--', label='background')\n",
    "        \n",
    "        # If it blows up, limit the y-axis\n",
    "        y_min, y_max = np.min(y), np.max(y)\n",
    "        y_range = y_max - y_min\n",
    "        plt.ylim(y_min - 0.1*y_range, y_max + 0.1*y_range)\n",
    "        plt.axvline(x=0,ls='--' ,alpha = 0.5, linewidth=1)\n",
    "\n",
    "        if peak == 0 and peak_time == 0:\n",
    "            pass\n",
    "        else:\n",
    "            plt.scatter(peak_time,peak, s=40, c='k', marker='*', label='peak')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 50), (51, 124)]\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9' already exists.\n",
      "delete C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9 y/n?\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9' was not removed.\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\test' already exists.\n",
      "delete C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\test y/n?\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\test' was not removed.\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\train' already exists.\n",
      "delete C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\train y/n?\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\train' was not removed.\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\other' already exists.\n",
      "delete C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\other y/n?\n",
      "Folder 'C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\other' was not removed.\n",
      "start\n",
      "total :  4374\n",
      "Data has been written to C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\params.json\n",
      "saving processed events to  C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 784/784 [00:00<00:00, 2786.54folder/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving processed events to  C:\\Users\\arpan\\OneDrive\\Documents\\GRB\\data\\DCL_ds_9\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing folders: 100%|██████████| 3200/3200 [54:51<00:00,  1.03s/folder]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time: 3292.15 seconds\n",
      "errors occured in:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    # name of the data set\n",
    "    source_data_set_path = r\"D:\\GRB_data\\500_data_set\"\n",
    "\n",
    "    # Get a list of all folders in the specified directory\n",
    "    folders = [str(folder) for folder in os.listdir(source_data_set_path) if os.path.isdir(os.path.join(source_data_set_path, folder))]\n",
    "\n",
    "    # list of bin sizes\n",
    "    bin_list =  [0.001,0.004,0.016,0.064,0.256,1.024,4.096] # [0.004,0.016,0.064,0.256,1.024,4.096] # [0.001, 0.005, 0.01, 0.1, 0.5, 1, 5] # \n",
    "\n",
    "    # number of datapoints in a light curve\n",
    "    data_no = 500\n",
    "\n",
    "    # ratio of pre-trigger to post-trigger\n",
    "    r = 0.5\n",
    "\n",
    "    # channel ranges\n",
    "    number_of_ranges = 8\n",
    "    size_of_ranges = int(128/number_of_ranges)\n",
    "    chrs = [(3,50),(51,124)]  # [(i, i + size_of_ranges-1) for i in range(0, 128, size_of_ranges)] #\n",
    "    print(chrs)\n",
    "    \n",
    "    # include unprocess data?\n",
    "    inup = True\n",
    "\n",
    "    dir_path = tools.json_path(r'data_path.json')\n",
    "    data_set_name = \"DCL_ds_9\"\n",
    "\n",
    "    # creating the data set folder\n",
    "    data_set_path = os.path.join(dir_path,data_set_name)\n",
    "    tools.create_folder(data_set_path,carefull=True) # carefull\n",
    "\n",
    "    # creating the test and train directories\n",
    "    test_path = os.path.join(data_set_path, 'test')\n",
    "    train_path = os.path.join(data_set_path, 'train')\n",
    "    other_path = os.path.join(data_set_path, 'other')\n",
    "    tools.create_folder(test_path,carefull=True)\n",
    "    tools.create_folder(train_path,carefull=True)\n",
    "    tools.create_folder(other_path,carefull=True)\n",
    "\n",
    "    print('start')\n",
    "    print('total : ',len(folders))\n",
    "\n",
    "    # Measure execution time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Writing the parameters to a json file\n",
    "    params_dict = {\"bin list\": bin_list, \"time interval\": 'n/a', \"number of data points\": data_no-1, \"data set name\": data_set_name, \"data set path\": str(data_set_path),\n",
    "                \"channel ranges\": chrs, \"ratio\":r, \"type\": \"hrates\", \"include unprocessed\": inup}\n",
    "    write_json_file = tools.write_json_file(params_dict, os.path.join(data_set_path,'params.json'))\n",
    "\n",
    "\n",
    "    error_folders = []\n",
    "\n",
    "    profiler = line_profiler.LineProfiler()\n",
    "    profiler.add_function(process_folder)\n",
    "    profiler.enable_by_count()\n",
    "\n",
    "zero_files = 0\n",
    "\n",
    "# processing test data\n",
    "print('saving processed events to ',test_path)\n",
    "for folder in tqdm(test_events[:], desc=\"Processing folders\", unit=\"folder\"):\n",
    "   zero_files = process_folder(folder,bin_list,data_no,r,chrs,test_path,source_data_set_path,error_folders,inup, zero_files,plot=False)    \n",
    "\n",
    "# processing train data\n",
    "print('saving processed events to ',train_path)\n",
    "for folder in tqdm(train_events[:], desc=\"Processing folders\", unit=\"folder\"):\n",
    "   zero_files = process_folder(folder,bin_list,data_no,r,chrs,train_path,source_data_set_path,error_folders,inup,zero_files, plot=False)    \n",
    "\n",
    "         \n",
    "# profiler.print_stats()\n",
    "# print('\\n----------------------------------------------------------------------------\\n\\nevents', folders, ' in folder', data_set_path)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "print(\"errors occured in:\")\n",
    "for folder in error_folders:\n",
    "    print(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(zero_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
